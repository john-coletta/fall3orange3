{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project Code\n",
    "\n",
    "#### Import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\johnb\\\\OneDrive\\\\Documents\\\\MSA\\\\Fall 3\\\\Machine Learning\\\\MLProjectData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat18</th>\n",
       "      <th>cat19</th>\n",
       "      <th>cat20</th>\n",
       "      <th>cat21</th>\n",
       "      <th>cat22</th>\n",
       "      <th>cat23</th>\n",
       "      <th>cat24</th>\n",
       "      <th>cat25</th>\n",
       "      <th>cat26</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1424.241</td>\n",
       "      <td>15.70</td>\n",
       "      <td>-1.930008</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>-0.014426</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>15.22</td>\n",
       "      <td>13.77</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983.787</td>\n",
       "      <td>15.59</td>\n",
       "      <td>0.190003</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>15.15</td>\n",
       "      <td>15.54</td>\n",
       "      <td>16.64</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.075</td>\n",
       "      <td>16.32</td>\n",
       "      <td>0.809998</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>16.23</td>\n",
       "      <td>16.71</td>\n",
       "      <td>17.44</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356.126</td>\n",
       "      <td>28.29</td>\n",
       "      <td>-1.409996</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>-0.047518</td>\n",
       "      <td>26.66</td>\n",
       "      <td>26.24</td>\n",
       "      <td>28.18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772.041</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.033136</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.97</td>\n",
       "      <td>10.74</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.400009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num1   num2      num3      num4      num5      num6      num7   num8  \\\n",
       "0  1424.241  15.70 -1.930008 -0.005399 -0.014426 -0.017944  0.011322  15.22   \n",
       "1   983.787  15.59  0.190003 -0.000129  0.002850  0.017414  0.033371  15.15   \n",
       "2   105.075  16.32  0.809998  0.000752  0.004340  0.025135  0.007954  16.23   \n",
       "3   356.126  28.29 -1.409996 -0.011557 -0.020185  0.003352 -0.047518  26.66   \n",
       "4   772.041  11.49  0.079987  0.001576  0.000724  0.025519  0.033136  11.76   \n",
       "\n",
       "    num9  num10    ...      cat18  cat19  cat20  cat21  cat22  cat23  cat24  \\\n",
       "0  13.77  13.96    ...      False   True  False  False  False  False  False   \n",
       "1  15.54  16.64    ...      False  False  False  False  False  False  False   \n",
       "2  16.71  17.44    ...      False  False  False  False  False  False  False   \n",
       "3  26.24  28.18    ...      False  False  False  False  False  False  False   \n",
       "4  11.97  10.74    ...      False  False  False  False  False  False  False   \n",
       "\n",
       "   cat25  cat26     target  \n",
       "0  False  False  21.639999  \n",
       "1  False  False  19.919998  \n",
       "2  False  False  21.150001  \n",
       "3  False  False  19.360001  \n",
       "4  False  False  21.400009  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B    1303\n",
      "C    1303\n",
      "D    1279\n",
      "E    1271\n",
      "A    1194\n",
      "Name: cat1, dtype: int64\n",
      "F    557\n",
      "H    554\n",
      "E    550\n",
      "J    550\n",
      "C    547\n",
      "L    529\n",
      "G    529\n",
      "D    527\n",
      "K    510\n",
      "A    509\n",
      "I    508\n",
      "B    480\n",
      "Name: cat2, dtype: int64\n",
      "False    6131\n",
      "True      219\n",
      "Name: cat3, dtype: int64\n",
      "False    6070\n",
      "True      280\n",
      "Name: cat4, dtype: int64\n",
      "False    6099\n",
      "True      251\n",
      "Name: cat5, dtype: int64\n",
      "False    6156\n",
      "True      194\n",
      "Name: cat6, dtype: int64\n",
      "False    6192\n",
      "True      158\n",
      "Name: cat7, dtype: int64\n",
      "False    6281\n",
      "True       69\n",
      "Name: cat8, dtype: int64\n",
      "False    6322\n",
      "True       28\n",
      "Name: cat9, dtype: int64\n",
      "False    6306\n",
      "True       44\n",
      "Name: cat10, dtype: int64\n",
      "False    6131\n",
      "True      219\n",
      "Name: cat11, dtype: int64\n",
      "False    6070\n",
      "True      280\n",
      "Name: cat12, dtype: int64\n",
      "False    6099\n",
      "True      251\n",
      "Name: cat13, dtype: int64\n",
      "False    6156\n",
      "True      194\n",
      "Name: cat14, dtype: int64\n",
      "False    6306\n",
      "True       44\n",
      "Name: cat15, dtype: int64\n",
      "False    6322\n",
      "True       28\n",
      "Name: cat16, dtype: int64\n",
      "False    6192\n",
      "True      158\n",
      "Name: cat17, dtype: int64\n",
      "False    6281\n",
      "True       69\n",
      "Name: cat18, dtype: int64\n",
      "False    6131\n",
      "True      219\n",
      "Name: cat19, dtype: int64\n",
      "False    6070\n",
      "True      280\n",
      "Name: cat20, dtype: int64\n",
      "False    6100\n",
      "True      250\n",
      "Name: cat21, dtype: int64\n",
      "False    6156\n",
      "True      194\n",
      "Name: cat22, dtype: int64\n",
      "False    6307\n",
      "True       43\n",
      "Name: cat23, dtype: int64\n",
      "False    6322\n",
      "True       28\n",
      "Name: cat24, dtype: int64\n",
      "False    6192\n",
      "True      158\n",
      "Name: cat25, dtype: int64\n",
      "False    6281\n",
      "True       69\n",
      "Name: cat26, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# See if any of the category have more than two levels\n",
    "for col in df.columns:\n",
    "    if col[0:3] == 'cat':\n",
    "        print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1_A</th>\n",
       "      <th>cat1_B</th>\n",
       "      <th>cat1_C</th>\n",
       "      <th>cat1_D</th>\n",
       "      <th>cat1_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1_A  cat1_B  cat1_C  cat1_D  cat1_E\n",
       "0       0       0       0       0       1\n",
       "1       1       0       0       0       0\n",
       "2       0       0       0       0       1\n",
       "3       0       0       1       0       0\n",
       "4       0       0       0       0       1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummies for cat1 variable\n",
    "dummies1 = pd.get_dummies(df['cat1'], prefix='cat1')\n",
    "dummies1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat2_A</th>\n",
       "      <th>cat2_B</th>\n",
       "      <th>cat2_C</th>\n",
       "      <th>cat2_D</th>\n",
       "      <th>cat2_E</th>\n",
       "      <th>cat2_F</th>\n",
       "      <th>cat2_G</th>\n",
       "      <th>cat2_H</th>\n",
       "      <th>cat2_I</th>\n",
       "      <th>cat2_J</th>\n",
       "      <th>cat2_K</th>\n",
       "      <th>cat2_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat2_A  cat2_B  cat2_C  cat2_D  cat2_E  cat2_F  cat2_G  cat2_H  cat2_I  \\\n",
       "0       0       0       0       1       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       1       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       1       0       0       0       0       0       0       0   \n",
       "\n",
       "   cat2_J  cat2_K  cat2_L  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       1       0  \n",
       "3       0       0       1  \n",
       "4       0       0       0  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummies for cat2 variable\n",
    "dummies2 = pd.get_dummies(df['cat2'], prefix='cat2')\n",
    "dummies2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data with the dummies and drop the original columns\n",
    "df_final = df.join(dummies1).join(dummies2)\n",
    "df_final.drop(['cat1','cat2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat2_C</th>\n",
       "      <th>cat2_D</th>\n",
       "      <th>cat2_E</th>\n",
       "      <th>cat2_F</th>\n",
       "      <th>cat2_G</th>\n",
       "      <th>cat2_H</th>\n",
       "      <th>cat2_I</th>\n",
       "      <th>cat2_J</th>\n",
       "      <th>cat2_K</th>\n",
       "      <th>cat2_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1424.241</td>\n",
       "      <td>15.70</td>\n",
       "      <td>-1.930008</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>-0.014426</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>15.22</td>\n",
       "      <td>13.77</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983.787</td>\n",
       "      <td>15.59</td>\n",
       "      <td>0.190003</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>15.15</td>\n",
       "      <td>15.54</td>\n",
       "      <td>16.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.075</td>\n",
       "      <td>16.32</td>\n",
       "      <td>0.809998</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>16.23</td>\n",
       "      <td>16.71</td>\n",
       "      <td>17.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356.126</td>\n",
       "      <td>28.29</td>\n",
       "      <td>-1.409996</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>-0.047518</td>\n",
       "      <td>26.66</td>\n",
       "      <td>26.24</td>\n",
       "      <td>28.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772.041</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.033136</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.97</td>\n",
       "      <td>10.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num1   num2      num3      num4      num5      num6      num7   num8  \\\n",
       "0  1424.241  15.70 -1.930008 -0.005399 -0.014426 -0.017944  0.011322  15.22   \n",
       "1   983.787  15.59  0.190003 -0.000129  0.002850  0.017414  0.033371  15.15   \n",
       "2   105.075  16.32  0.809998  0.000752  0.004340  0.025135  0.007954  16.23   \n",
       "3   356.126  28.29 -1.409996 -0.011557 -0.020185  0.003352 -0.047518  26.66   \n",
       "4   772.041  11.49  0.079987  0.001576  0.000724  0.025519  0.033136  11.76   \n",
       "\n",
       "    num9  num10   ...    cat2_C  cat2_D  cat2_E  cat2_F  cat2_G  cat2_H  \\\n",
       "0  13.77  13.96   ...         0       1       0       0       0       0   \n",
       "1  15.54  16.64   ...         0       0       0       0       1       0   \n",
       "2  16.71  17.44   ...         0       0       0       0       0       0   \n",
       "3  26.24  28.18   ...         0       0       0       0       0       0   \n",
       "4  11.97  10.74   ...         0       0       0       0       0       0   \n",
       "\n",
       "   cat2_I  cat2_J  cat2_K  cat2_L  \n",
       "0       0       0       0       0  \n",
       "1       0       0       0       0  \n",
       "2       0       0       1       0  \n",
       "3       0       0       0       1  \n",
       "4       0       0       0       0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variable and target subsets\n",
    "var = df_final.drop('target',axis=1)\n",
    "target = df_final['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic OLS Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9602879158667785"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performe a train-test split and calculate the baseline MAE\n",
    "var_train, var_test, target_train, target_test = train_test_split(var, target, test_size=0.3, random_state=69)\n",
    "baseline = [target_train.mean()] * len(target_test)\n",
    "base_score = mean_absolute_error(target_test, baseline)\n",
    "base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the basic linear model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(var_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017441789738017466"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R-squared\n",
    "reg.score(var_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006227093595627897"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "preds = reg.predict(var_test)\n",
    "OLS_mae = mean_absolute_error(target_test, preds)\n",
    "# Difference between baseline and OLS\n",
    "base_score - OLS_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reguarlized Regression (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the reguarlized regression model\n",
    "lasso = linear_model.Lasso()\n",
    "lasso.fit(var_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000613759129573177"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions and score\n",
    "lasso_preds = lasso.predict(var_test)\n",
    "lasso_mae = mean_absolute_error(target_test, lasso_preds)\n",
    "base_score - lasso_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let us see how ridge does\n",
    "ridge = linear_model.Ridge()\n",
    "ridge.fit(var_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006661489761288553"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_preds = ridge.predict(var_test)\n",
    "ridge_mae = mean_absolute_error(target_test, ridge_preds)\n",
    "base_score - ridge_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's grid search over elastic net\n",
    "# so that we get the optimal ratio\n",
    "en = linear_model.ElasticNet(max_iter=5000, random_state=69)\n",
    "param_grid = {'l1_ratio':np.arange(0,1.1,0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=5000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=69, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=6,\n",
       "       param_grid={'l1_ratio': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the grid search\n",
    "en_grid = GridSearchCV(en, param_grid, scoring='neg_mean_absolute_error', n_jobs=6, cv=5)\n",
    "en_grid.fit(var, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Output the results to a dataframe\n",
    "en_df = pd.DataFrame(en_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'l1_ratio': 0.0}</td>\n",
       "      <td>-0.959481</td>\n",
       "      <td>-0.953606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'l1_ratio': 0.1}</td>\n",
       "      <td>-0.957484</td>\n",
       "      <td>-0.954409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'l1_ratio': 0.2}</td>\n",
       "      <td>-0.956560</td>\n",
       "      <td>-0.954610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'l1_ratio': 0.30000000000000004}</td>\n",
       "      <td>-0.956371</td>\n",
       "      <td>-0.954849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'l1_ratio': 0.4}</td>\n",
       "      <td>-0.956375</td>\n",
       "      <td>-0.954952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'l1_ratio': 0.5}</td>\n",
       "      <td>-0.956402</td>\n",
       "      <td>-0.955020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'l1_ratio': 0.6000000000000001}</td>\n",
       "      <td>-0.956384</td>\n",
       "      <td>-0.955083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'l1_ratio': 0.7000000000000001}</td>\n",
       "      <td>-0.956337</td>\n",
       "      <td>-0.955160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'l1_ratio': 0.8}</td>\n",
       "      <td>-0.956304</td>\n",
       "      <td>-0.955251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'l1_ratio': 0.9}</td>\n",
       "      <td>-0.956276</td>\n",
       "      <td>-0.955321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'l1_ratio': 1.0}</td>\n",
       "      <td>-0.956257</td>\n",
       "      <td>-0.955376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               params  mean_test_score  mean_train_score\n",
       "0                   {'l1_ratio': 0.0}        -0.959481         -0.953606\n",
       "1                   {'l1_ratio': 0.1}        -0.957484         -0.954409\n",
       "2                   {'l1_ratio': 0.2}        -0.956560         -0.954610\n",
       "3   {'l1_ratio': 0.30000000000000004}        -0.956371         -0.954849\n",
       "4                   {'l1_ratio': 0.4}        -0.956375         -0.954952\n",
       "5                   {'l1_ratio': 0.5}        -0.956402         -0.955020\n",
       "6    {'l1_ratio': 0.6000000000000001}        -0.956384         -0.955083\n",
       "7    {'l1_ratio': 0.7000000000000001}        -0.956337         -0.955160\n",
       "8                   {'l1_ratio': 0.8}        -0.956304         -0.955251\n",
       "9                   {'l1_ratio': 0.9}        -0.956276         -0.955321\n",
       "10                  {'l1_ratio': 1.0}        -0.956257         -0.955376"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df[['params','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'l1_ratio': 1.0}</td>\n",
       "      <td>-0.956257</td>\n",
       "      <td>-0.955376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               params  mean_test_score  mean_train_score\n",
       "10  {'l1_ratio': 1.0}        -0.956257         -0.955376"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the best result\n",
    "en_df[en_df['mean_test_score'] == en_df['mean_test_score'].max()][['params','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was our best result so we decided to go with this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('C:\\\\Users\\\\johnb\\\\OneDrive\\\\Documents\\\\MSA\\\\Fall 3\\\\Machine Learning\\\\testData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "      <th>cat19</th>\n",
       "      <th>cat20</th>\n",
       "      <th>cat21</th>\n",
       "      <th>cat22</th>\n",
       "      <th>cat23</th>\n",
       "      <th>cat24</th>\n",
       "      <th>cat25</th>\n",
       "      <th>cat26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6403</td>\n",
       "      <td>569.259</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.539978</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6404</td>\n",
       "      <td>664.937</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6405</td>\n",
       "      <td>505.504</td>\n",
       "      <td>12.69</td>\n",
       "      <td>3.790009</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6406</td>\n",
       "      <td>519.668</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.579987</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6407</td>\n",
       "      <td>770.547</td>\n",
       "      <td>13.63</td>\n",
       "      <td>-1.559998</td>\n",
       "      <td>-0.007314</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>12.64</td>\n",
       "      <td>12.69</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     num1   num2      num3      num4      num5      num6  \\\n",
       "0        6403  569.259  14.97  0.539978  0.008158  0.004598  0.007786   \n",
       "1        6404  664.937  13.37  1.750000  0.008458  0.016685  0.014065   \n",
       "2        6405  505.504  12.69  3.790009  0.009004  0.017539  0.031743   \n",
       "3        6406  519.668  12.64  1.579987  0.003598  0.012635  0.029569   \n",
       "4        6407  770.547  13.63 -1.559998 -0.007314 -0.003742  0.020569   \n",
       "\n",
       "       num7   num8   num9  ...    cat17  cat18  cat19  cat20  cat21  cat22  \\\n",
       "0 -0.020163  16.14  15.60  ...    False  False  False  False  False  False   \n",
       "1 -0.006170  14.97  16.14  ...    False  False  False  False  False  False   \n",
       "2  0.004845  13.37  14.97  ...    False  False  False  False  False  False   \n",
       "3  0.012341  12.69  13.37  ...    False  False  False  False  False  False   \n",
       "4  0.003225  12.64  12.69  ...    False  False  False  False  False  False   \n",
       "\n",
       "   cat23  cat24  cat25  cat26  \n",
       "0  False  False  False  False  \n",
       "1  False  False  False  False  \n",
       "2  False  False  False  False  \n",
       "3  False  False  False  False  \n",
       "4  False  False  False  False  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_row = test_data['Unnamed: 0']\n",
    "test_data_var = test_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "      <th>cat19</th>\n",
       "      <th>cat20</th>\n",
       "      <th>cat21</th>\n",
       "      <th>cat22</th>\n",
       "      <th>cat23</th>\n",
       "      <th>cat24</th>\n",
       "      <th>cat25</th>\n",
       "      <th>cat26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569.259</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.539978</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>16.09</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>664.937</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505.504</td>\n",
       "      <td>12.69</td>\n",
       "      <td>3.790009</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519.668</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.579987</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>770.547</td>\n",
       "      <td>13.63</td>\n",
       "      <td>-1.559998</td>\n",
       "      <td>-0.007314</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>12.64</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num1   num2      num3      num4      num5      num6      num7   num8  \\\n",
       "0  569.259  14.97  0.539978  0.008158  0.004598  0.007786 -0.020163  16.14   \n",
       "1  664.937  13.37  1.750000  0.008458  0.016685  0.014065 -0.006170  14.97   \n",
       "2  505.504  12.69  3.790009  0.009004  0.017539  0.031743  0.004845  13.37   \n",
       "3  519.668  12.64  1.579987  0.003598  0.012635  0.029569  0.012341  12.69   \n",
       "4  770.547  13.63 -1.559998 -0.007314 -0.003742  0.020569  0.003225  12.64   \n",
       "\n",
       "    num9  num10  ...    cat17  cat18  cat19  cat20  cat21  cat22  cat23  \\\n",
       "0  15.60  16.09  ...    False  False  False  False  False  False  False   \n",
       "1  16.14  15.60  ...    False  False  False  False  False  False  False   \n",
       "2  14.97  16.14  ...    False  False  False  False  False  False  False   \n",
       "3  13.37  14.97  ...    False  False  False  False  False  False  False   \n",
       "4  12.69  13.37  ...    False  False  False  False  False  False  False   \n",
       "\n",
       "   cat24  cat25  cat26  \n",
       "0  False  False  False  \n",
       "1  False  False  False  \n",
       "2  False  False  False  \n",
       "3  False  False  False  \n",
       "4  False  False  False  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies1_test = pd.get_dummies(test_data_var['cat1'], prefix='cat1')\n",
    "dummies2_test = pd.get_dummies(test_data_var['cat2'], prefix='cat2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = test_data_var.join(dummies1_test).join(dummies2_test)\n",
    "test_final.drop(['cat1','cat2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat26</th>\n",
       "      <th>cat1_A</th>\n",
       "      <th>cat1_B</th>\n",
       "      <th>cat1_C</th>\n",
       "      <th>cat1_D</th>\n",
       "      <th>cat1_E</th>\n",
       "      <th>cat2_G</th>\n",
       "      <th>cat2_H</th>\n",
       "      <th>cat2_I</th>\n",
       "      <th>cat2_J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569.259</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.539978</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>16.09</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>664.937</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505.504</td>\n",
       "      <td>12.69</td>\n",
       "      <td>3.790009</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519.668</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.579987</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>770.547</td>\n",
       "      <td>13.63</td>\n",
       "      <td>-1.559998</td>\n",
       "      <td>-0.007314</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>12.64</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num1   num2      num3      num4      num5      num6      num7   num8  \\\n",
       "0  569.259  14.97  0.539978  0.008158  0.004598  0.007786 -0.020163  16.14   \n",
       "1  664.937  13.37  1.750000  0.008458  0.016685  0.014065 -0.006170  14.97   \n",
       "2  505.504  12.69  3.790009  0.009004  0.017539  0.031743  0.004845  13.37   \n",
       "3  519.668  12.64  1.579987  0.003598  0.012635  0.029569  0.012341  12.69   \n",
       "4  770.547  13.63 -1.559998 -0.007314 -0.003742  0.020569  0.003225  12.64   \n",
       "\n",
       "    num9  num10   ...    cat26  cat1_A  cat1_B  cat1_C  cat1_D  cat1_E  \\\n",
       "0  15.60  16.09   ...    False       0       0       0       1       0   \n",
       "1  16.14  15.60   ...    False       0       0       0       0       1   \n",
       "2  14.97  16.14   ...    False       1       0       0       0       0   \n",
       "3  13.37  14.97   ...    False       0       1       0       0       0   \n",
       "4  12.69  13.37   ...    False       0       0       1       0       0   \n",
       "\n",
       "   cat2_G  cat2_H  cat2_I  cat2_J  \n",
       "0       1       0       0       0  \n",
       "1       1       0       0       0  \n",
       "2       1       0       0       0  \n",
       "3       1       0       0       0  \n",
       "4       1       0       0       0  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in var.columns:\n",
    "    if col not in test_final.columns:\n",
    "        test_final[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>cat2_I</th>\n",
       "      <th>cat2_J</th>\n",
       "      <th>cat2_A</th>\n",
       "      <th>cat2_B</th>\n",
       "      <th>cat2_C</th>\n",
       "      <th>cat2_D</th>\n",
       "      <th>cat2_E</th>\n",
       "      <th>cat2_F</th>\n",
       "      <th>cat2_K</th>\n",
       "      <th>cat2_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569.259</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.539978</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>16.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>664.937</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>15.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505.504</td>\n",
       "      <td>12.69</td>\n",
       "      <td>3.790009</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.031743</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519.668</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.579987</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>14.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>770.547</td>\n",
       "      <td>13.63</td>\n",
       "      <td>-1.559998</td>\n",
       "      <td>-0.007314</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>12.64</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num1   num2      num3      num4      num5      num6      num7   num8  \\\n",
       "0  569.259  14.97  0.539978  0.008158  0.004598  0.007786 -0.020163  16.14   \n",
       "1  664.937  13.37  1.750000  0.008458  0.016685  0.014065 -0.006170  14.97   \n",
       "2  505.504  12.69  3.790009  0.009004  0.017539  0.031743  0.004845  13.37   \n",
       "3  519.668  12.64  1.579987  0.003598  0.012635  0.029569  0.012341  12.69   \n",
       "4  770.547  13.63 -1.559998 -0.007314 -0.003742  0.020569  0.003225  12.64   \n",
       "\n",
       "    num9  num10   ...    cat2_I  cat2_J  cat2_A  cat2_B  cat2_C  cat2_D  \\\n",
       "0  15.60  16.09   ...         0       0       0       0       0       0   \n",
       "1  16.14  15.60   ...         0       0       0       0       0       0   \n",
       "2  14.97  16.14   ...         0       0       0       0       0       0   \n",
       "3  13.37  14.97   ...         0       0       0       0       0       0   \n",
       "4  12.69  13.37   ...         0       0       0       0       0       0   \n",
       "\n",
       "   cat2_E  cat2_F  cat2_K  cat2_L  \n",
       "0       0       0       0       0  \n",
       "1       0       0       0       0  \n",
       "2       0       0       0       0  \n",
       "3       0       0       0       0  \n",
       "4       0       0       0       0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = linear_model.ElasticNet(max_iter=5000, l1_ratio=1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=1,\n",
       "      max_iter=5000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=69, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(var, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = final_model.predict(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df = pd.DataFrame(final_preds).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.083064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.074197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20.047319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20.038862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.049601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          0\n",
       "0      0  20.083064\n",
       "1      1  20.074197\n",
       "2      2  20.047319\n",
       "3      3  20.038862\n",
       "4      4  20.049601"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df.columns = ['Row','Prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.083064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.074197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20.047319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20.038862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.049601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row  Prediction\n",
       "0    0   20.083064\n",
       "1    1   20.074197\n",
       "2    2   20.047319\n",
       "3    3   20.038862\n",
       "4    4   20.049601"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df['Row'] = final_pred_df['Row'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20.083064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20.074197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20.047319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20.038862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20.049601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row  Prediction\n",
       "0    1   20.083064\n",
       "1    2   20.074197\n",
       "2    3   20.047319\n",
       "3    4   20.038862\n",
       "4    5   20.049601"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df.to_csv('C:\\\\Users\\\\johnb\\\\OneDrive\\\\Documents\\\\MSA\\\\Fall 3\\\\Machine Learning\\\\Orange3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and use cross-validation to score\n",
    "tree = DecisionTreeRegressor()\n",
    "cv = cross_validate(tree, var, target, scoring='neg_mean_absolute_error', return_train_score=True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.468004  , -1.41615268, -1.41121311, -1.46245574, -1.38853182])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.25590551e-07, -7.58038058e-07, -7.39304462e-07, -4.58825459e-07,\n",
       "       -3.84383202e-07])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv['train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples_split': [2, 5, 10, 20], 'min_samples_leaf': [1, 2, 5, 8, 10, 20, 30, 40]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the parameters for grid search\n",
    "tree_params = {'max_depth': [i+1 for i in range(10)], 'min_samples_split' :[2,5,10,20], 'min_samples_leaf':[1,2,5,8,10,20,30,40]}\n",
    "grid = GridSearchCV(tree, param_grid=tree_params,scoring='neg_mean_absolute_error', cv=5)\n",
    "grid.fit(var, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005318568231099108"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score off the best model\n",
    "grid_df = pd.DataFrame(grid.cv_results_)\n",
    "base_score - (-1 * grid_df['mean_test_score'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>-0.95312</td>\n",
       "      <td>-0.963466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954969</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.950908</td>\n",
       "      <td>-0.948507</td>\n",
       "      <td>-0.956669</td>\n",
       "      <td>-0.947486</td>\n",
       "      <td>-0.950721</td>\n",
       "      <td>-0.950858</td>\n",
       "      <td>0.003184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "46        0.09057      0.003052         0.007169         0.00074   \n",
       "\n",
       "   param_max_depth param_min_samples_leaf param_min_samples_split  \\\n",
       "46               2                      8                      10   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "46  {'max_depth': 2, 'min_samples_leaf': 8, 'min_s...           -0.95312   \n",
       "\n",
       "    split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "46          -0.963466       ...               -0.954969        0.013405   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "46                1           -0.950908           -0.948507   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "46           -0.956669           -0.947486           -0.950721   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "46         -0.950858         0.003184  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df[grid_df['mean_test_score']==grid_df['mean_test_score'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  62.301276814585435\n"
     ]
    }
   ],
   "source": [
    "# Set up and fit random forest\n",
    "start = timeit.default_timer()\n",
    "test_forest = RandomForestRegressor(n_estimators=500, n_jobs=6, random_state=69)\n",
    "test_forest.fit(var_train, target_train)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('time: ',stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018224755261102565"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions and score\n",
    "test_forest_preds = test_forest.predict(var_test)\n",
    "test_forest_mae = mean_absolute_error(target_test, test_forest_preds)\n",
    "base_score - test_forest_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  369.8960190282014\n"
     ]
    }
   ],
   "source": [
    "# Also run with cross-validation\n",
    "start = timeit.default_timer()\n",
    "cv_forest = cross_validate(test_forest, var, target, scoring=['neg_mean_absolute_error','neg_mean_squared_error'], return_train_score=True, cv=5)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('time: ',stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36292071 -0.36276081 -0.36489346 -0.36403172 -0.364787  ]\n",
      "[-0.29044786 -0.29130447 -0.29886637 -0.29534025 -0.30701109]\n"
     ]
    }
   ],
   "source": [
    "print(cv_forest['train_neg_mean_absolute_error'])\n",
    "print(cv_forest['train_neg_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.98300467 -0.98199264 -0.95950743 -0.99118755 -0.98190623]\n",
      "[-2.18072445 -2.23154712 -1.9485578  -2.2361245  -1.95170937]\n"
     ]
    }
   ],
   "source": [
    "print(cv_forest['test_neg_mean_absolute_error'])\n",
    "print(cv_forest['test_neg_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up forest for grid search\n",
    "forest = RandomForestRegressor(random_state=69)\n",
    "forest_params = {'n_estimators':[10,100,1000], 'max_features':['auto','log2',10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_search = GridSearchCV(forest, param_grid=forest_params, scoring=['neg_mean_absolute_error','neg_mean_squared_error'], cv=5, n_jobs=6, refit='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  903.3776962496559\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "### WARNING ###\n",
    "## This takes like 15 minutes ##\n",
    "\n",
    "start = timeit.default_timer()\n",
    "forest_search.fit(var, target)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('time: ', stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_neg_mean_absolute_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_neg_mean_squared_error'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "rand_df = pd.DataFrame(forest_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009982061413150611"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best score\n",
    "base_score - (-1 * rand_df['mean_test_neg_mean_absolute_error'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_neg_mean_absolute_error</th>\n",
       "      <th>split1_test_neg_mean_absolute_error</th>\n",
       "      <th>split2_test_neg_mean_absolute_error</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>std_test_neg_mean_squared_error</th>\n",
       "      <th>rank_test_neg_mean_squared_error</th>\n",
       "      <th>split0_train_neg_mean_squared_error</th>\n",
       "      <th>split1_train_neg_mean_squared_error</th>\n",
       "      <th>split2_train_neg_mean_squared_error</th>\n",
       "      <th>split3_train_neg_mean_squared_error</th>\n",
       "      <th>split4_train_neg_mean_squared_error</th>\n",
       "      <th>mean_train_neg_mean_squared_error</th>\n",
       "      <th>std_train_neg_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53.939423</td>\n",
       "      <td>1.07597</td>\n",
       "      <td>5.885934</td>\n",
       "      <td>0.261703</td>\n",
       "      <td>log2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_features': 'log2', 'n_estimators': 1000}</td>\n",
       "      <td>-0.980622</td>\n",
       "      <td>-0.967729</td>\n",
       "      <td>-0.952132</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.067634</td>\n",
       "      <td>0.119914</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.276363</td>\n",
       "      <td>-0.277391</td>\n",
       "      <td>-0.287194</td>\n",
       "      <td>-0.277478</td>\n",
       "      <td>-0.287935</td>\n",
       "      <td>-0.281272</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5      53.939423       1.07597         5.885934        0.261703   \n",
       "\n",
       "  param_max_features param_n_estimators  \\\n",
       "5               log2               1000   \n",
       "\n",
       "                                           params  \\\n",
       "5  {'max_features': 'log2', 'n_estimators': 1000}   \n",
       "\n",
       "   split0_test_neg_mean_absolute_error  split1_test_neg_mean_absolute_error  \\\n",
       "5                            -0.980622                            -0.967729   \n",
       "\n",
       "   split2_test_neg_mean_absolute_error                ...                 \\\n",
       "5                            -0.952132                ...                  \n",
       "\n",
       "   mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  \\\n",
       "5                         -2.067634                         0.119914   \n",
       "\n",
       "   rank_test_neg_mean_squared_error  split0_train_neg_mean_squared_error  \\\n",
       "5                                 1                            -0.276363   \n",
       "\n",
       "   split1_train_neg_mean_squared_error  split2_train_neg_mean_squared_error  \\\n",
       "5                            -0.277391                            -0.287194   \n",
       "\n",
       "   split3_train_neg_mean_squared_error  split4_train_neg_mean_squared_error  \\\n",
       "5                            -0.277478                            -0.287935   \n",
       "\n",
       "   mean_train_neg_mean_squared_error  std_train_neg_mean_squared_error  \n",
       "5                          -0.281272                          0.005158  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_df[rand_df['mean_test_neg_mean_absolute_error'] == rand_df['mean_test_neg_mean_absolute_error'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering with FeatureTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the entities\n",
    "es = ft.EntitySet(id='data')\n",
    "es = es.entity_from_dataframe(entity_id='data',\n",
    "                             dataframe=var, \n",
    "                              index='index',\n",
    "                             make_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable: index (dtype = index)>,\n",
       " <Variable: num1 (dtype = numeric)>,\n",
       " <Variable: num2 (dtype = numeric)>,\n",
       " <Variable: num3 (dtype = numeric)>,\n",
       " <Variable: num4 (dtype = numeric)>,\n",
       " <Variable: num5 (dtype = numeric)>,\n",
       " <Variable: num6 (dtype = numeric)>,\n",
       " <Variable: num7 (dtype = numeric)>,\n",
       " <Variable: num8 (dtype = numeric)>,\n",
       " <Variable: num9 (dtype = numeric)>,\n",
       " <Variable: num10 (dtype = numeric)>,\n",
       " <Variable: num11 (dtype = numeric)>,\n",
       " <Variable: num12 (dtype = numeric)>,\n",
       " <Variable: num13 (dtype = numeric)>,\n",
       " <Variable: num14 (dtype = numeric)>,\n",
       " <Variable: num15 (dtype = numeric)>,\n",
       " <Variable: num16 (dtype = numeric)>,\n",
       " <Variable: num17 (dtype = numeric)>,\n",
       " <Variable: num18 (dtype = numeric)>,\n",
       " <Variable: num19 (dtype = numeric)>,\n",
       " <Variable: num20 (dtype = numeric)>,\n",
       " <Variable: num21 (dtype = numeric)>,\n",
       " <Variable: num22 (dtype = numeric)>,\n",
       " <Variable: num23 (dtype = numeric)>,\n",
       " <Variable: num24 (dtype = numeric)>,\n",
       " <Variable: num25 (dtype = numeric)>,\n",
       " <Variable: num26 (dtype = numeric)>,\n",
       " <Variable: num27 (dtype = numeric)>,\n",
       " <Variable: num28 (dtype = numeric)>,\n",
       " <Variable: num29 (dtype = numeric)>,\n",
       " <Variable: num30 (dtype = numeric)>,\n",
       " <Variable: num31 (dtype = numeric)>,\n",
       " <Variable: num32 (dtype = numeric)>,\n",
       " <Variable: num33 (dtype = numeric)>,\n",
       " <Variable: num34 (dtype = numeric)>,\n",
       " <Variable: num35 (dtype = numeric)>,\n",
       " <Variable: num36 (dtype = numeric)>,\n",
       " <Variable: num37 (dtype = numeric)>,\n",
       " <Variable: num38 (dtype = numeric)>,\n",
       " <Variable: num39 (dtype = numeric)>,\n",
       " <Variable: num40 (dtype = numeric)>,\n",
       " <Variable: num41 (dtype = numeric)>,\n",
       " <Variable: num42 (dtype = numeric)>,\n",
       " <Variable: num43 (dtype = numeric)>,\n",
       " <Variable: num44 (dtype = numeric)>,\n",
       " <Variable: num45 (dtype = numeric)>,\n",
       " <Variable: num46 (dtype = numeric)>,\n",
       " <Variable: num47 (dtype = numeric)>,\n",
       " <Variable: num48 (dtype = numeric)>,\n",
       " <Variable: num49 (dtype = numeric)>,\n",
       " <Variable: num50 (dtype = numeric)>,\n",
       " <Variable: num51 (dtype = numeric)>,\n",
       " <Variable: num52 (dtype = numeric)>,\n",
       " <Variable: num53 (dtype = numeric)>,\n",
       " <Variable: num54 (dtype = numeric)>,\n",
       " <Variable: num55 (dtype = numeric)>,\n",
       " <Variable: num56 (dtype = numeric)>,\n",
       " <Variable: num57 (dtype = numeric)>,\n",
       " <Variable: num58 (dtype = numeric)>,\n",
       " <Variable: num59 (dtype = numeric)>,\n",
       " <Variable: cat3 (dtype = boolean)>,\n",
       " <Variable: cat4 (dtype = boolean)>,\n",
       " <Variable: cat5 (dtype = boolean)>,\n",
       " <Variable: cat6 (dtype = boolean)>,\n",
       " <Variable: cat7 (dtype = boolean)>,\n",
       " <Variable: cat8 (dtype = boolean)>,\n",
       " <Variable: cat9 (dtype = boolean)>,\n",
       " <Variable: cat10 (dtype = boolean)>,\n",
       " <Variable: cat11 (dtype = boolean)>,\n",
       " <Variable: cat12 (dtype = boolean)>,\n",
       " <Variable: cat13 (dtype = boolean)>,\n",
       " <Variable: cat14 (dtype = boolean)>,\n",
       " <Variable: cat15 (dtype = boolean)>,\n",
       " <Variable: cat16 (dtype = boolean)>,\n",
       " <Variable: cat17 (dtype = boolean)>,\n",
       " <Variable: cat18 (dtype = boolean)>,\n",
       " <Variable: cat19 (dtype = boolean)>,\n",
       " <Variable: cat20 (dtype = boolean)>,\n",
       " <Variable: cat21 (dtype = boolean)>,\n",
       " <Variable: cat22 (dtype = boolean)>,\n",
       " <Variable: cat23 (dtype = boolean)>,\n",
       " <Variable: cat24 (dtype = boolean)>,\n",
       " <Variable: cat25 (dtype = boolean)>,\n",
       " <Variable: cat26 (dtype = boolean)>,\n",
       " <Variable: cat1_A (dtype = numeric)>,\n",
       " <Variable: cat1_B (dtype = numeric)>,\n",
       " <Variable: cat1_C (dtype = numeric)>,\n",
       " <Variable: cat1_D (dtype = numeric)>,\n",
       " <Variable: cat1_E (dtype = numeric)>,\n",
       " <Variable: cat2_A (dtype = numeric)>,\n",
       " <Variable: cat2_B (dtype = numeric)>,\n",
       " <Variable: cat2_C (dtype = numeric)>,\n",
       " <Variable: cat2_D (dtype = numeric)>,\n",
       " <Variable: cat2_E (dtype = numeric)>,\n",
       " <Variable: cat2_F (dtype = numeric)>,\n",
       " <Variable: cat2_G (dtype = numeric)>,\n",
       " <Variable: cat2_H (dtype = numeric)>,\n",
       " <Variable: cat2_I (dtype = numeric)>,\n",
       " <Variable: cat2_J (dtype = numeric)>,\n",
       " <Variable: cat2_K (dtype = numeric)>,\n",
       " <Variable: cat2_L (dtype = numeric)>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es['data'].variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_true</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the number of 'True' values in a boolean.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Counts the number of non null values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg_time_between</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the average time between consecutive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time_since_last</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Time since last related instance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skew</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the skewness of a data set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mode</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the most common element in a categorical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Computes the average value of a numeric feature.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_unique</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Returns the number of unique categorical varia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the maximum non-null value of a numeric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trend</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Calculates the slope of the linear trend of va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sum</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Sums elements of a numeric or boolean feature.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>all</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Test if all values are 'True'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>last</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Returns the last value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>percent_true</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the percent of 'True' values in a boolea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n_most_common</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the N most common elements in a categori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>any</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Test if any value is 'True'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>median</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the median value of any feature with wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>min</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the minimum non-null value of a numeric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>std</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>Finds the standard deviation of a numeric feat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>latitude</td>\n",
       "      <td>transform</td>\n",
       "      <td>Returns the first value of the tuple base feat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>isin</td>\n",
       "      <td>transform</td>\n",
       "      <td>For each value of the base feature, checks whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cum_min</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the min of previous values of an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>multiply</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that multplies two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time_since</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates time since the cutoff time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>week</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mod</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that divides two f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>add</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that adds two feat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>year</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>absolute</td>\n",
       "      <td>transform</td>\n",
       "      <td>Absolute value of base feature.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cum_sum</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the sum of previous values of an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cum_max</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the max of previous values of an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>longitude</td>\n",
       "      <td>transform</td>\n",
       "      <td>Returns the second value on the tuple base fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>minutes</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>days</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>not</td>\n",
       "      <td>transform</td>\n",
       "      <td>For each value of the base feature, negates th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>second</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the second.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>divide</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that divides two f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>weekend</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform Datetime feature into the boolean of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>hours</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>time_since_previous</td>\n",
       "      <td>transform</td>\n",
       "      <td>Compute the time since the previous instance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>years</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>days_since</td>\n",
       "      <td>transform</td>\n",
       "      <td>For each value of the base feature, compute th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>cum_mean</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the mean of previous values of an i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>and</td>\n",
       "      <td>transform</td>\n",
       "      <td>For two boolean values, determine if both valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>or</td>\n",
       "      <td>transform</td>\n",
       "      <td>For two boolean values, determine if one value...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>subtract</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that subtracts two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>weeks</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hour</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the hour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>minute</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the minute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>numwords</td>\n",
       "      <td>transform</td>\n",
       "      <td>Returns the words in a given string by countin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>is_null</td>\n",
       "      <td>transform</td>\n",
       "      <td>For each value of base feature, return 'True' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>negate</td>\n",
       "      <td>transform</td>\n",
       "      <td>Creates a transform feature that negates a fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>weekday</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform Datetime feature into the boolean of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>characters</td>\n",
       "      <td>transform</td>\n",
       "      <td>Return the characters in a given string.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>percentile</td>\n",
       "      <td>transform</td>\n",
       "      <td>For each value of the base feature, determines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>seconds</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>cum_count</td>\n",
       "      <td>transform</td>\n",
       "      <td>Calculates the number of previous values of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>months</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Timedelta feature into the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>diff</td>\n",
       "      <td>transform</td>\n",
       "      <td>Compute the difference between the value of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>day</td>\n",
       "      <td>transform</td>\n",
       "      <td>Transform a Datetime feature into the day.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name         type  \\\n",
       "0              num_true  aggregation   \n",
       "1                 count  aggregation   \n",
       "2      avg_time_between  aggregation   \n",
       "3       time_since_last  aggregation   \n",
       "4                  skew  aggregation   \n",
       "5                  mode  aggregation   \n",
       "6                  mean  aggregation   \n",
       "7            num_unique  aggregation   \n",
       "8                   max  aggregation   \n",
       "9                 trend  aggregation   \n",
       "10                  sum  aggregation   \n",
       "11                  all  aggregation   \n",
       "12                 last  aggregation   \n",
       "13         percent_true  aggregation   \n",
       "14        n_most_common  aggregation   \n",
       "15                  any  aggregation   \n",
       "16               median  aggregation   \n",
       "17                  min  aggregation   \n",
       "18                  std  aggregation   \n",
       "19             latitude    transform   \n",
       "20                 isin    transform   \n",
       "21              cum_min    transform   \n",
       "22             multiply    transform   \n",
       "23           time_since    transform   \n",
       "24                 week    transform   \n",
       "25                  mod    transform   \n",
       "26                  add    transform   \n",
       "27                 year    transform   \n",
       "28             absolute    transform   \n",
       "29              cum_sum    transform   \n",
       "..                  ...          ...   \n",
       "32              cum_max    transform   \n",
       "33            longitude    transform   \n",
       "34              minutes    transform   \n",
       "35                 days    transform   \n",
       "36                  not    transform   \n",
       "37               second    transform   \n",
       "38               divide    transform   \n",
       "39              weekend    transform   \n",
       "40                hours    transform   \n",
       "41  time_since_previous    transform   \n",
       "42                years    transform   \n",
       "43           days_since    transform   \n",
       "44             cum_mean    transform   \n",
       "45                  and    transform   \n",
       "46                   or    transform   \n",
       "47             subtract    transform   \n",
       "48                weeks    transform   \n",
       "49                 hour    transform   \n",
       "50               minute    transform   \n",
       "51             numwords    transform   \n",
       "52              is_null    transform   \n",
       "53               negate    transform   \n",
       "54              weekday    transform   \n",
       "55           characters    transform   \n",
       "56           percentile    transform   \n",
       "57              seconds    transform   \n",
       "58            cum_count    transform   \n",
       "59               months    transform   \n",
       "60                 diff    transform   \n",
       "61                  day    transform   \n",
       "\n",
       "                                          description  \n",
       "0     Finds the number of 'True' values in a boolean.  \n",
       "1               Counts the number of non null values.  \n",
       "2   Computes the average time between consecutive ...  \n",
       "3                   Time since last related instance.  \n",
       "4                Computes the skewness of a data set.  \n",
       "5   Finds the most common element in a categorical...  \n",
       "6    Computes the average value of a numeric feature.  \n",
       "7   Returns the number of unique categorical varia...  \n",
       "8   Finds the maximum non-null value of a numeric ...  \n",
       "9   Calculates the slope of the linear trend of va...  \n",
       "10     Sums elements of a numeric or boolean feature.  \n",
       "11                     Test if all values are 'True'.  \n",
       "12                            Returns the last value.  \n",
       "13  Finds the percent of 'True' values in a boolea...  \n",
       "14  Finds the N most common elements in a categori...  \n",
       "15                       Test if any value is 'True'.  \n",
       "16  Finds the median value of any feature with wel...  \n",
       "17  Finds the minimum non-null value of a numeric ...  \n",
       "18  Finds the standard deviation of a numeric feat...  \n",
       "19  Returns the first value of the tuple base feat...  \n",
       "20  For each value of the base feature, checks whe...  \n",
       "21  Calculates the min of previous values of an in...  \n",
       "22  Creates a transform feature that multplies two...  \n",
       "23             Calculates time since the cutoff time.  \n",
       "24        Transform a Datetime feature into the week.  \n",
       "25  Creates a transform feature that divides two f...  \n",
       "26  Creates a transform feature that adds two feat...  \n",
       "27        Transform a Datetime feature into the year.  \n",
       "28                    Absolute value of base feature.  \n",
       "29  Calculates the sum of previous values of an in...  \n",
       "..                                                ...  \n",
       "32  Calculates the max of previous values of an in...  \n",
       "33  Returns the second value on the tuple base fea...  \n",
       "34  Transform a Timedelta feature into the number ...  \n",
       "35  Transform a Timedelta feature into the number ...  \n",
       "36  For each value of the base feature, negates th...  \n",
       "37      Transform a Datetime feature into the second.  \n",
       "38  Creates a transform feature that divides two f...  \n",
       "39  Transform Datetime feature into the boolean of...  \n",
       "40  Transform a Timedelta feature into the number ...  \n",
       "41      Compute the time since the previous instance.  \n",
       "42  Transform a Timedelta feature into the number ...  \n",
       "43  For each value of the base feature, compute th...  \n",
       "44  Calculates the mean of previous values of an i...  \n",
       "45  For two boolean values, determine if both valu...  \n",
       "46  For two boolean values, determine if one value...  \n",
       "47  Creates a transform feature that subtracts two...  \n",
       "48  Transform a Timedelta feature into the number ...  \n",
       "49        Transform a Datetime feature into the hour.  \n",
       "50      Transform a Datetime feature into the minute.  \n",
       "51  Returns the words in a given string by countin...  \n",
       "52  For each value of base feature, return 'True' ...  \n",
       "53  Creates a transform feature that negates a fea...  \n",
       "54  Transform Datetime feature into the boolean of...  \n",
       "55           Return the characters in a given string.  \n",
       "56  For each value of the base feature, determines...  \n",
       "57  Transform a Timedelta feature into the number ...  \n",
       "58  Calculates the number of previous values of an...  \n",
       "59  Transform a Timedelta feature into the number ...  \n",
       "60  Compute the difference between the value of a ...  \n",
       "61         Transform a Datetime feature into the day.  \n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.list_primitives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to workers in 4.075 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Exception in Future <Future cancelled> after timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 970, in error_callback\n",
      "    future.result()\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:57395 remote=tcp://127.0.0.1:57330>\n"
     ]
    }
   ],
   "source": [
    "# Create the features using transformation primitives\n",
    "feature_matrix, feature_defs = ft.dfs(entityset=es,\n",
    "                                     target_entity='data',\n",
    "                                     trans_primitives=['multiply','add',\n",
    "                                                      'subtract'],\n",
    "                                     n_jobs=6,\n",
    "                                     max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>num22 - num50</th>\n",
       "      <th>num27 - num24</th>\n",
       "      <th>num29 - cat1_C</th>\n",
       "      <th>num21 - num16</th>\n",
       "      <th>cat2_L - num1</th>\n",
       "      <th>num12 - cat2_J</th>\n",
       "      <th>cat2_F - num8</th>\n",
       "      <th>cat1_C - cat2_D</th>\n",
       "      <th>num39 - num53</th>\n",
       "      <th>num6 - num8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1424.241</td>\n",
       "      <td>15.70</td>\n",
       "      <td>-1.930008</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>-0.014426</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>15.22</td>\n",
       "      <td>13.77</td>\n",
       "      <td>13.96</td>\n",
       "      <td>...</td>\n",
       "      <td>115.629465</td>\n",
       "      <td>-8.854600</td>\n",
       "      <td>58.836975</td>\n",
       "      <td>-1.249988</td>\n",
       "      <td>-1424.241</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>-15.22</td>\n",
       "      <td>255</td>\n",
       "      <td>49.605040</td>\n",
       "      <td>-15.237944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>983.787</td>\n",
       "      <td>15.59</td>\n",
       "      <td>0.190003</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>15.15</td>\n",
       "      <td>15.54</td>\n",
       "      <td>16.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-113.504117</td>\n",
       "      <td>-31.733729</td>\n",
       "      <td>61.478762</td>\n",
       "      <td>0.229997</td>\n",
       "      <td>-983.787</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>-15.15</td>\n",
       "      <td>0</td>\n",
       "      <td>27.626613</td>\n",
       "      <td>-15.132586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105.075</td>\n",
       "      <td>16.32</td>\n",
       "      <td>0.809998</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>16.23</td>\n",
       "      <td>16.71</td>\n",
       "      <td>17.44</td>\n",
       "      <td>...</td>\n",
       "      <td>20.019640</td>\n",
       "      <td>-30.229271</td>\n",
       "      <td>48.888235</td>\n",
       "      <td>-0.610004</td>\n",
       "      <td>-105.075</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>-16.23</td>\n",
       "      <td>0</td>\n",
       "      <td>17.826410</td>\n",
       "      <td>-16.204865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356.126</td>\n",
       "      <td>28.29</td>\n",
       "      <td>-1.409996</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>-0.047518</td>\n",
       "      <td>26.66</td>\n",
       "      <td>26.24</td>\n",
       "      <td>28.18</td>\n",
       "      <td>...</td>\n",
       "      <td>57.637477</td>\n",
       "      <td>-38.815950</td>\n",
       "      <td>37.987422</td>\n",
       "      <td>-2.949999</td>\n",
       "      <td>-355.126</td>\n",
       "      <td>0.061140</td>\n",
       "      <td>-26.66</td>\n",
       "      <td>1</td>\n",
       "      <td>28.791835</td>\n",
       "      <td>-26.656648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772.041</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.033136</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.97</td>\n",
       "      <td>10.74</td>\n",
       "      <td>...</td>\n",
       "      <td>50.890634</td>\n",
       "      <td>-26.785792</td>\n",
       "      <td>77.661471</td>\n",
       "      <td>0.510003</td>\n",
       "      <td>-772.041</td>\n",
       "      <td>-0.022959</td>\n",
       "      <td>-11.76</td>\n",
       "      <td>0</td>\n",
       "      <td>29.890500</td>\n",
       "      <td>-11.734481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 11500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           num1   num2      num3      num4      num5      num6      num7  \\\n",
       "index                                                                      \n",
       "0      1424.241  15.70 -1.930008 -0.005399 -0.014426 -0.017944  0.011322   \n",
       "1       983.787  15.59  0.190003 -0.000129  0.002850  0.017414  0.033371   \n",
       "2       105.075  16.32  0.809998  0.000752  0.004340  0.025135  0.007954   \n",
       "3       356.126  28.29 -1.409996 -0.011557 -0.020185  0.003352 -0.047518   \n",
       "4       772.041  11.49  0.079987  0.001576  0.000724  0.025519  0.033136   \n",
       "\n",
       "        num8   num9  num10     ...       num22 - num50  num27 - num24  \\\n",
       "index                          ...                                      \n",
       "0      15.22  13.77  13.96     ...          115.629465      -8.854600   \n",
       "1      15.15  15.54  16.64     ...         -113.504117     -31.733729   \n",
       "2      16.23  16.71  17.44     ...           20.019640     -30.229271   \n",
       "3      26.66  26.24  28.18     ...           57.637477     -38.815950   \n",
       "4      11.76  11.97  10.74     ...           50.890634     -26.785792   \n",
       "\n",
       "       num29 - cat1_C  num21 - num16  cat2_L - num1  num12 - cat2_J  \\\n",
       "index                                                                 \n",
       "0           58.836975      -1.249988      -1424.241        0.031537   \n",
       "1           61.478762       0.229997       -983.787        0.029043   \n",
       "2           48.888235      -0.610004       -105.075        0.005545   \n",
       "3           37.987422      -2.949999       -355.126        0.061140   \n",
       "4           77.661471       0.510003       -772.041       -0.022959   \n",
       "\n",
       "       cat2_F - num8  cat1_C - cat2_D  num39 - num53  num6 - num8  \n",
       "index                                                              \n",
       "0             -15.22              255      49.605040   -15.237944  \n",
       "1             -15.15                0      27.626613   -15.132586  \n",
       "2             -16.23                0      17.826410   -16.204865  \n",
       "3             -26.66                1      28.791835   -26.656648  \n",
       "4             -11.76                0      29.890500   -11.734481  \n",
       "\n",
       "[5 rows x 11500 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  752.5145476308046\n"
     ]
    }
   ],
   "source": [
    "# Run a random forest with all the new features\n",
    "\n",
    "### WARNING ###\n",
    "## This also takes a while ##\n",
    "\n",
    "start = timeit.default_timer()\n",
    "eng_forest = RandomForestRegressor(n_estimators=1000, n_jobs=6, max_features='log2', random_state=69)\n",
    "cv_eng_forest = cross_validate(eng_forest, feature_matrix, target, scoring=['neg_mean_absolute_error','neg_mean_squared_error'], return_train_score=True, cv=5)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('time: ',stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9602879158667785\n",
      "[-0.98092445 -0.97044855 -0.95605318 -0.97716734 -0.96985617]\n",
      "[-2.21397017 -2.14677232 -1.94055349 -2.17917173 -1.91123632]\n"
     ]
    }
   ],
   "source": [
    "#Get the best scores\n",
    "print(base_score)\n",
    "print(cv_eng_forest['test_neg_mean_absolute_error'])\n",
    "print(cv_eng_forest['test_neg_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-b49ebaa27be4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Split the feature matrix into training and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mft_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mft_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m69\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the feature matrix into training and test\n",
    "ft_train, ft_test, lab_train, lab_test = train_test_split(feature_matrix, target, test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data in proper form for XGBoost\n",
    "dtrain = xg.DMatrix(var_train, label=target_train)\n",
    "dtest = xg.DMatrix(var_test, label=target_test)\n",
    "\n",
    "#fttrain = xg.DMatrix(ft_train, label=lab_train)\n",
    "#fttest = xg.DMatrix(ft_test, label=lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters for XGBoost model\n",
    "xg_param = {'max_depth':3,'nthread':6,'silent':1,'objective':'reg:linear'}\n",
    "xg_param['eval_metric'] = 'mae'\n",
    "evallist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "#ftevallist = [(fttrain, 'train'),(fttest,'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:13.6986\teval-mae:13.5937\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[1]\ttrain-mae:9.59086\teval-mae:9.4856\n",
      "[2]\ttrain-mae:6.71978\teval-mae:6.61379\n",
      "[3]\ttrain-mae:4.71698\teval-mae:4.61891\n",
      "[4]\ttrain-mae:3.33493\teval-mae:3.24593\n",
      "[5]\ttrain-mae:2.40733\teval-mae:2.34393\n",
      "[6]\ttrain-mae:1.8091\teval-mae:1.77012\n",
      "[7]\ttrain-mae:1.4373\teval-mae:1.42513\n",
      "[8]\ttrain-mae:1.21362\teval-mae:1.22362\n",
      "[9]\ttrain-mae:1.08034\teval-mae:1.10837\n",
      "[10]\ttrain-mae:1.00564\teval-mae:1.04084\n",
      "[11]\ttrain-mae:0.961942\teval-mae:1.00336\n",
      "[12]\ttrain-mae:0.936595\teval-mae:0.983398\n",
      "[13]\ttrain-mae:0.920036\teval-mae:0.976162\n",
      "[14]\ttrain-mae:0.910682\teval-mae:0.971434\n",
      "[15]\ttrain-mae:0.904132\teval-mae:0.969761\n",
      "[16]\ttrain-mae:0.898466\teval-mae:0.972156\n",
      "[17]\ttrain-mae:0.894102\teval-mae:0.969855\n",
      "[18]\ttrain-mae:0.892687\teval-mae:0.969927\n",
      "[19]\ttrain-mae:0.890171\teval-mae:0.970159\n",
      "[20]\ttrain-mae:0.888305\teval-mae:0.970488\n",
      "[21]\ttrain-mae:0.884061\teval-mae:0.971692\n",
      "[22]\ttrain-mae:0.879726\teval-mae:0.973101\n",
      "[23]\ttrain-mae:0.874545\teval-mae:0.973187\n",
      "[24]\ttrain-mae:0.871313\teval-mae:0.973675\n",
      "[25]\ttrain-mae:0.869803\teval-mae:0.973203\n",
      "[26]\ttrain-mae:0.866242\teval-mae:0.973299\n",
      "[27]\ttrain-mae:0.863855\teval-mae:0.973894\n",
      "[28]\ttrain-mae:0.861453\teval-mae:0.97338\n",
      "[29]\ttrain-mae:0.860594\teval-mae:0.974478\n",
      "[30]\ttrain-mae:0.856785\teval-mae:0.974878\n",
      "[31]\ttrain-mae:0.855274\teval-mae:0.975265\n",
      "[32]\ttrain-mae:0.852301\teval-mae:0.976322\n",
      "[33]\ttrain-mae:0.848523\teval-mae:0.978063\n",
      "[34]\ttrain-mae:0.847178\teval-mae:0.977551\n",
      "[35]\ttrain-mae:0.844412\teval-mae:0.977619\n",
      "[36]\ttrain-mae:0.842124\teval-mae:0.978474\n",
      "[37]\ttrain-mae:0.839404\teval-mae:0.979205\n",
      "[38]\ttrain-mae:0.83748\teval-mae:0.97958\n",
      "[39]\ttrain-mae:0.834295\teval-mae:0.979262\n",
      "[40]\ttrain-mae:0.830856\teval-mae:0.980947\n",
      "[41]\ttrain-mae:0.828737\teval-mae:0.981911\n",
      "[42]\ttrain-mae:0.825898\teval-mae:0.982817\n",
      "[43]\ttrain-mae:0.823294\teval-mae:0.983021\n",
      "[44]\ttrain-mae:0.821031\teval-mae:0.984061\n",
      "[45]\ttrain-mae:0.820055\teval-mae:0.984897\n",
      "[46]\ttrain-mae:0.817599\teval-mae:0.985328\n",
      "[47]\ttrain-mae:0.815182\teval-mae:0.986624\n",
      "[48]\ttrain-mae:0.812976\teval-mae:0.987324\n",
      "[49]\ttrain-mae:0.811801\teval-mae:0.986742\n",
      "[50]\ttrain-mae:0.80833\teval-mae:0.986947\n",
      "[51]\ttrain-mae:0.807236\teval-mae:0.986342\n",
      "[52]\ttrain-mae:0.806065\teval-mae:0.987571\n",
      "[53]\ttrain-mae:0.802889\teval-mae:0.988404\n",
      "[54]\ttrain-mae:0.800734\teval-mae:0.988092\n",
      "[55]\ttrain-mae:0.798801\teval-mae:0.987479\n",
      "[56]\ttrain-mae:0.796809\teval-mae:0.986981\n",
      "[57]\ttrain-mae:0.793428\teval-mae:0.986514\n",
      "[58]\ttrain-mae:0.790493\teval-mae:0.988406\n",
      "[59]\ttrain-mae:0.787424\teval-mae:0.988945\n",
      "[60]\ttrain-mae:0.784052\teval-mae:0.989254\n",
      "[61]\ttrain-mae:0.782291\teval-mae:0.989305\n",
      "[62]\ttrain-mae:0.780635\teval-mae:0.98932\n",
      "[63]\ttrain-mae:0.77849\teval-mae:0.989432\n",
      "[64]\ttrain-mae:0.776938\teval-mae:0.990048\n",
      "[65]\ttrain-mae:0.775989\teval-mae:0.989594\n",
      "Stopping. Best iteration:\n",
      "[15]\ttrain-mae:0.904132\teval-mae:0.969761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model on original variables\n",
    "num_round = 100\n",
    "bst = xg.train(xg_param, dtrain, num_round, evallist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:13.6989\teval-mae:13.5929\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mae:9.59066\teval-mae:9.4833\n",
      "[2]\ttrain-mae:6.71582\teval-mae:6.61375\n",
      "[3]\ttrain-mae:4.71308\teval-mae:4.6175\n",
      "[4]\ttrain-mae:3.33203\teval-mae:3.2485\n",
      "[5]\ttrain-mae:2.40287\teval-mae:2.34645\n",
      "[6]\ttrain-mae:1.80759\teval-mae:1.77214\n",
      "[7]\ttrain-mae:1.43494\teval-mae:1.42519\n",
      "[8]\ttrain-mae:1.20912\teval-mae:1.22178\n",
      "[9]\ttrain-mae:1.07773\teval-mae:1.10076\n",
      "[10]\ttrain-mae:1.0039\teval-mae:1.03401\n",
      "[11]\ttrain-mae:0.962453\teval-mae:0.998721\n",
      "[12]\ttrain-mae:0.939167\teval-mae:0.980303\n",
      "[13]\ttrain-mae:0.926497\teval-mae:0.970994\n",
      "[14]\ttrain-mae:0.918274\teval-mae:0.968065\n",
      "[15]\ttrain-mae:0.911569\teval-mae:0.969079\n",
      "[16]\ttrain-mae:0.90788\teval-mae:0.968331\n",
      "[17]\ttrain-mae:0.904295\teval-mae:0.968882\n",
      "[18]\ttrain-mae:0.900551\teval-mae:0.968958\n",
      "[19]\ttrain-mae:0.89863\teval-mae:0.968938\n",
      "[20]\ttrain-mae:0.895318\teval-mae:0.969214\n",
      "[21]\ttrain-mae:0.893214\teval-mae:0.969857\n",
      "[22]\ttrain-mae:0.891112\teval-mae:0.969804\n",
      "[23]\ttrain-mae:0.888222\teval-mae:0.971598\n",
      "[24]\ttrain-mae:0.885841\teval-mae:0.970271\n",
      "Stopping. Best iteration:\n",
      "[14]\ttrain-mae:0.918274\teval-mae:0.968065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model on all new variables\n",
    "#ftbst = xg.train(xg_param, fttrain, num_round, ftevallist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_preds = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f412f4d780>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xg.plot_importance(bst);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f41775e0f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(var_train['num3'],target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab just the numeric columns\n",
    "nums = [col for col in df_final.columns if col[0:3] == 'num']\n",
    "num_train = var_train[nums]\n",
    "num_test = var_test[nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the scaled numeric variables\n",
    "pca = PCA(n_components=20).fit_transform(scale(num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the components through a linear model\n",
    "pca_reg = linear_model.LinearRegression()\n",
    "pca_score = cross_validate(pca_reg, pca, target_train, scoring=['neg_mean_absolute_error','neg_mean_squared_error'], return_train_score=True, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.00581836 -0.97985907 -0.89378028 -0.93595191 -0.97842187]\n",
      "[-2.42612447 -2.15790599 -1.67023449 -1.78273828 -2.01951272]\n"
     ]
    }
   ],
   "source": [
    "print(pca_score['test_neg_mean_absolute_error'])\n",
    "print(pca_score['test_neg_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Regression with New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add squared numeric variables\n",
    "new_var = var\n",
    "for col in var.columns:\n",
    "    if col[0:3] == 'num':\n",
    "        new_var[col + '_squared'] = (var[col] ** 2)\n",
    "        new_var[col] = var[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the numeric and categorical variables\n",
    "numeric = []\n",
    "cat = []\n",
    "for col in var.columns:\n",
    "    if col[0:3] == 'num':\n",
    "        numeric.append(col)\n",
    "        \n",
    "for col in var.columns:\n",
    "    if col[0:3] == 'cat':\n",
    "        cat.append(col)\n",
    "        \n",
    "num_vars = var[numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More feature engineering\n",
    "es_lasso = ft.EntitySet(id='data')\n",
    "es_lasso = es_lasso.entity_from_dataframe(entity_id='data',\n",
    "                             dataframe=num_vars, \n",
    "                              index='index',\n",
    "                             make_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to workers in 6.245 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Exception in Future <Future cancelled> after timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 970, in error_callback\n",
      "    future.result()\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_lasso, feature_defs_lasso = ft.dfs(entityset=es_lasso,\n",
    "                                     target_entity='data',\n",
    "                                     trans_primitives=['multiply'],\n",
    "                                     n_jobs=6,\n",
    "                                     max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7021"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_defs_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\johnb\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Scale the data for the reguarlized regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "lasso_nums = scaler.fit_transform(feature_matrix_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57430188, -0.45754513, -1.42642275, ...,  0.92802377,\n",
       "        -0.11711891,  0.58175377],\n",
       "       [ 0.13390894, -0.47105767,  0.11211609, ...,  0.59985859,\n",
       "        -0.12654008, -0.23913302],\n",
       "       [-0.74468123, -0.38138356,  0.56206017, ...,  0.19013016,\n",
       "        -0.14937287, -0.22388131],\n",
       "       ...,\n",
       "       [ 0.17049687, -0.33470389, -0.67892971, ..., -0.46363508,\n",
       "        -0.06294178,  0.5350589 ],\n",
       "       [-0.79592613, -0.53862035,  0.14433743, ..., -0.20133555,\n",
       "        -0.14162026, -0.21798028],\n",
       "       [ 1.75663394,  2.40711265,  0.67092429, ..., -0.04569434,\n",
       "         0.03852244, -0.12490063]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57430188, -0.45754513, -1.42642275, ..., -0.18321983,\n",
       "         0.17433701,  1.67855846],\n",
       "       [ 0.13390894, -0.47105767,  0.11211609, ..., -0.19771363,\n",
       "        -0.3025507 , -0.0431972 ],\n",
       "       [-0.74468123, -0.38138356,  0.56206017, ..., -0.43168647,\n",
       "        -0.32877641, -0.51846052],\n",
       "       ...,\n",
       "       [ 0.17049687, -0.33470389, -0.67892971, ...,  1.66265387,\n",
       "        -0.23544807,  1.5543326 ],\n",
       "       [-0.79592613, -0.53862035,  0.14433743, ..., -0.43647586,\n",
       "        -0.26994034, -1.11805176],\n",
       "       [ 1.75663394,  2.40711265,  0.67092429, ..., -0.26343793,\n",
       "         0.71330336,  0.680163  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split again\n",
    "feature_train, feature_test, y_train, y_test = train_test_split(lasso_nums, target, test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models and predict and score\n",
    "lasso_ft = linear_model.Lasso().fit(feature_train, y_train)\n",
    "ridge_ft = linear_model.Ridge().fit(feature_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_preds = lasso_ft.predict(feature_test)\n",
    "ridge_preds = ridge_ft.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9602879158667785 0.9602879158667768 1.7081414039391034\n"
     ]
    }
   ],
   "source": [
    "lasso_score = mean_absolute_error(lasso_preds, y_test)\n",
    "ridge_score = mean_absolute_error(ridge_preds, y_test)\n",
    "print(base_score, lasso_score, ridge_score)\n",
    "# Scores aren't great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nerual net time!!!\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = var_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the neural network\n",
    "nnet = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh activation is standard\n",
    "nnet.add(Dense(64, activation='tanh', input_shape = (n_cols,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.add(Dense(32, activation='tanh'))\n",
    "nnet.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet_optimize(net, optimizers):\n",
    "    '''This function is supposed to be a grid search like optimizer\n",
    "    for neural networks. Basically it just runs with different optimizers and \n",
    "    returns the best score and which optimizer produced it. Theoretically it\n",
    "    could be modified to run over the different parameters.'''\n",
    "    best_score = -1\n",
    "    param = ''\n",
    "    early_stopping = EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        bst = ModelCheckpoint(filepath = optimizer + '.mdl.hdf5', save_best_only=True, monitor='mean_absolute_error')\n",
    "        net.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n",
    "        net.fit(var_train, target_train, epochs=1000, verbose=0, callbacks=[early_stopping,bst])\n",
    "        net.load_weights(filepath='.mdl.hdf5')\n",
    "        nnet_preds = net.predict(var_test)\n",
    "        nnet_score = mean_absolute_error(target_test, nnet_preds)\n",
    "        print(nnet_score)\n",
    "        if best_score==-1:\n",
    "            best_score = nnet_score\n",
    "            \n",
    "            param = optimizer\n",
    "        else:\n",
    "            if nnet_score < best_score:\n",
    "                best_score = nnet_score\n",
    "                param = optimizer\n",
    "    return param, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4445/4445 [==============================] - 1s 120us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 2/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 3/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 4/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 5/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 6/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 7/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 8/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 9/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 10/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 11/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 12/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9540 - mean_absolute_error: 0.9540: 0s - loss: 0.9652 - mean_absolute_error: 0.9\n",
      "Epoch 13/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 14/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 15/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 16/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 17/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 18/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 19/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 20/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 21/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 22/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 23/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9552 - mean_absolute_error: 0.9552: 0s - loss: 0.9571 - mean_absolute_error: 0.9\n",
      "Epoch 24/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 25/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 26/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 27/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 28/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 29/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 30/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 31/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 32/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 33/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 34/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 35/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 36/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 37/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 38/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 39/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 40/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 41/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 42/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 43/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9536 - mean_absolute_error: 0.9536: 0s - loss: 0.9297 - mean_absolute_error: 0.\n",
      "Epoch 44/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 45/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 46/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 47/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 48/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 49/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 50/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 51/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 52/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 53/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 54/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 55/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 56/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 57/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 58/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 59/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 60/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 61/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 62/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 63/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 64/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 65/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 66/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 67/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 68/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 69/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 70/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 71/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 72/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 73/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 74/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 75/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 76/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 77/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 78/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 79/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 80/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 81/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 82/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 83/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 84/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 85/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 86/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 87/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 88/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 89/1000\n",
      "4445/4445 [==============================] - 0s 37us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 90/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 91/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 92/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 93/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 94/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 95/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 96/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 97/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 98/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 99/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 100/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 101/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 102/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 103/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 104/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 105/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 106/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 107/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 108/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 109/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 110/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 111/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 112/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 113/1000\n",
      "4445/4445 [==============================] - 0s 37us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 114/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 115/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 116/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 117/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 118/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 119/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 120/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 121/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 122/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 123/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 124/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 125/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 126/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 127/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 128/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 129/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 130/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 131/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 132/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 133/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 134/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 135/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 136/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 137/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 139/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 140/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 141/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 142/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 143/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 144/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9514 - mean_absolute_error: 0.9514: 0s - loss: 0.9781 - mean_absolute_error: 0.\n",
      "Epoch 145/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 146/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 147/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 148/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 149/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 150/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 151/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 152/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 153/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 154/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 155/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 156/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 157/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 158/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "0.9605127046427088\n",
      "Epoch 1/1000\n",
      "4445/4445 [==============================] - 1s 119us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 2/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 3/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 4/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 5/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 6/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 7/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 8/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 9/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 10/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 11/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 12/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 13/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 14/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 15/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 16/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 17/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 18/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 19/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 20/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 21/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 22/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 23/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 24/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 25/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 26/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 27/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 28/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 29/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 30/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 31/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 32/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 33/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 34/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 35/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 36/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 37/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 38/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 39/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 40/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 41/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 42/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 43/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 44/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 45/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 46/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 47/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 48/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 50/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 51/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 52/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 53/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 54/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 55/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 56/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 57/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 58/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 59/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 60/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 61/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 62/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 63/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 64/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 65/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 66/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 67/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 68/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 69/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 70/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 71/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 72/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 73/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 74/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 75/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 76/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 77/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 78/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 79/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 80/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 81/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 82/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 83/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 84/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 85/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 86/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 87/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 88/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 89/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 90/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 91/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 92/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 93/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 94/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 95/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 96/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 97/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 98/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 99/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 100/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 101/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 102/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 103/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 104/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 105/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 106/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 107/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 108/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 109/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 110/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 111/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 112/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 113/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 114/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 115/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 116/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 117/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 118/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 120/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 121/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 122/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 123/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 124/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 125/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 126/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 127/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 128/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 129/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 130/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 131/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 132/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 133/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 134/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 135/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 136/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 137/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 138/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 139/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 140/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 141/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 142/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 143/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 144/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 145/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 146/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 147/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 148/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 149/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 150/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 151/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 152/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 153/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 154/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 155/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 156/1000\n",
      "4445/4445 [==============================] - ETA: 0s - loss: 0.9369 - mean_absolute_error: 0.936 - 0s 52us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 157/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 158/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 159/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 160/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 161/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 162/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 163/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 164/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 165/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 166/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9534 - mean_absolute_error: 0.9534: 0s - loss: 0.9814 - mean_absolute_error: 0.9\n",
      "Epoch 167/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 168/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 169/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 170/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 171/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 172/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 173/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9525 - mean_absolute_error: 0.9525\n",
      "Epoch 174/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 175/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 176/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 177/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 178/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 179/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 180/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 181/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 182/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 183/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 184/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 185/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 186/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 188/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 189/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "0.9750210028326721\n",
      "Epoch 1/1000\n",
      "4445/4445 [==============================] - 1s 134us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 2/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 3/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 4/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 5/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 6/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 7/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 8/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 9/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9569 - mean_absolute_error: 0.9569\n",
      "Epoch 10/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9563 - mean_absolute_error: 0.9563\n",
      "Epoch 11/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 12/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 13/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9571 - mean_absolute_error: 0.9571\n",
      "Epoch 14/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 15/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9566 - mean_absolute_error: 0.9566: 0s - loss: 0.9499 - mean_absolute_error: 0.949\n",
      "Epoch 16/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 17/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 18/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 19/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 20/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 21/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 22/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9582 - mean_absolute_error: 0.9582\n",
      "Epoch 23/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 24/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 25/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 26/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 27/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 28/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 29/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9567 - mean_absolute_error: 0.9567\n",
      "Epoch 30/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 31/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 32/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9573 - mean_absolute_error: 0.9573\n",
      "Epoch 33/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9563 - mean_absolute_error: 0.9563\n",
      "Epoch 34/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 35/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 36/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 37/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 38/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 39/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 40/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 41/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 42/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 43/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 44/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 45/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 46/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9570 - mean_absolute_error: 0.9570\n",
      "Epoch 47/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 48/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 49/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 50/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9571 - mean_absolute_error: 0.9571\n",
      "Epoch 51/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 52/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 53/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 54/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 55/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 56/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 57/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9566 - mean_absolute_error: 0.9566\n",
      "Epoch 58/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 59/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 60/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 61/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 62/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 63/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 64/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 65/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 66/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 68/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 69/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 70/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 71/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 72/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9567 - mean_absolute_error: 0.9567\n",
      "Epoch 73/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 74/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 75/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 76/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 77/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 78/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 79/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 80/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 81/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 82/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 83/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 84/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 85/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 86/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 87/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 88/1000\n",
      "4445/4445 [==============================] - 0s 37us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 89/1000\n",
      "4445/4445 [==============================] - 0s 37us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 90/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 91/1000\n",
      "4445/4445 [==============================] - 0s 37us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 92/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 93/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 94/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 95/1000\n",
      "4445/4445 [==============================] - 0s 38us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 96/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 97/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 98/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 99/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 100/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 101/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 102/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 103/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 104/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 105/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 106/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 107/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 108/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 109/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 110/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 111/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 112/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 113/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 114/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 115/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9570 - mean_absolute_error: 0.9570\n",
      "Epoch 116/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 117/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 118/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 119/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 120/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 121/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 122/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 123/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 124/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 125/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9541 - mean_absolute_error: 0.9541: 0s - loss: 0.9422 - mean_absolute_error: 0.942\n",
      "Epoch 126/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 127/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "Epoch 128/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 129/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 130/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 131/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 132/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 133/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9568 - mean_absolute_error: 0.9568\n",
      "Epoch 134/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 135/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 137/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 138/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9543 - mean_absolute_error: 0.9543: 0s - loss: 0.9571 - mean_absolute_error: 0.957\n",
      "Epoch 139/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 140/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 141/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "Epoch 142/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 143/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 144/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 145/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 146/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9559 - mean_absolute_error: 0.9559: 0s - loss: 0.9507 - mean_absolute_error: 0.95\n",
      "Epoch 147/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 148/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9564 - mean_absolute_error: 0.9564\n",
      "Epoch 149/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 150/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 151/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 152/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 153/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 154/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 155/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 156/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 157/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 158/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 159/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 160/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 161/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9569 - mean_absolute_error: 0.9569\n",
      "Epoch 162/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 163/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 164/1000\n",
      "4445/4445 [==============================] - 0s 39us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 165/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 166/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 167/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 168/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 169/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 170/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 171/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 172/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 173/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 174/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 175/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 176/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 177/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 178/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 179/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 180/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 181/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 182/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 183/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 184/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 185/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 186/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 187/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 188/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 189/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9564 - mean_absolute_error: 0.9564\n",
      "Epoch 190/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 191/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 192/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 193/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 194/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9564 - mean_absolute_error: 0.9564\n",
      "Epoch 195/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 196/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 197/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 198/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 199/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9556 - mean_absolute_error: 0.9556: 0s - loss: 0.9596 - mean_absolute_error: 0.959\n",
      "0.9750210028326721\n",
      "Epoch 1/1000\n",
      "4445/4445 [==============================] - 1s 137us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 2/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 3/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 4/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 5/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 6/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 7/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9533 - mean_absolute_error: 0.9533\n",
      "Epoch 8/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9552 - mean_absolute_error: 0.9552: 0s - loss: 0.9673 - mean_absolute_error: 0.967\n",
      "Epoch 9/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 10/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 11/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 12/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 13/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 14/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 15/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 16/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 17/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 18/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 19/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 20/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 21/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 22/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 23/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9547 - mean_absolute_error: 0.9547: 0s - loss: 0.9800 - mean_absolute_error: 0.9\n",
      "Epoch 24/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 25/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 26/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 27/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 28/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "Epoch 29/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 30/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 31/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 32/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 33/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 34/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 35/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 36/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 37/1000\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 38/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 39/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 40/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 41/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 42/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9552 - mean_absolute_error: 0.9552: 0s - loss: 0.9280 - mean_absolute_error: 0.\n",
      "Epoch 43/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 44/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 45/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 46/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 47/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 48/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 49/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 50/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 51/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 52/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 53/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 54/1000\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 55/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 56/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 57/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 58/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 59/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 60/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 61/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 62/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 63/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 64/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 65/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9568 - mean_absolute_error: 0.9568\n",
      "Epoch 66/1000\n",
      "4445/4445 [==============================] - 0s 40us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 67/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 68/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 69/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 70/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 71/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 72/1000\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 73/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/1000\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 75/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9563 - mean_absolute_error: 0.9563\n",
      "Epoch 76/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 77/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9566 - mean_absolute_error: 0.9566\n",
      "Epoch 78/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 79/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 80/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 81/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 82/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9550 - mean_absolute_error: 0.9550: 0s - loss: 0.9607 - mean_absolute_error: 0.960\n",
      "Epoch 83/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 84/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 85/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 86/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 87/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 88/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 89/1000\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 90/1000\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 91/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 92/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 93/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 94/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 95/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 96/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 97/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 98/1000\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 99/1000\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 100/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 101/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 102/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 103/1000\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 104/1000\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 105/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 106/1000\n",
      "4445/4445 [==============================] - 0s 42us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 107/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 108/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 109/1000\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 110/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 111/1000\n",
      "4445/4445 [==============================] - 0s 55us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 112/1000\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 113/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9554 - mean_absolute_error: 0.9554: 0s - loss: 0.9580 - mean_absolute_error: 0.958\n",
      "Epoch 114/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 115/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 116/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 117/1000\n",
      "4445/4445 [==============================] - 0s 41us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 118/1000\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "0.9750210028326721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('adam', 0.9605127046427088)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function\n",
    "nnet_optimize(nnet, ['adam','adadelta','sgd','nadam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4445/4445 [==============================] - 1s 204us/step - loss: 8.8632 - mean_absolute_error: 8.8632\n",
      "Epoch 2/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 1.3162 - mean_absolute_error: 1.3162\n",
      "Epoch 3/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 4/100\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9549 - mean_absolute_error: 0.9549: 0s - loss: 0.9439 - mean_absolute_error: 0.94\n",
      "Epoch 5/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 6/100\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 7/100\n",
      "4445/4445 [==============================] - 0s 51us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 8/100\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 9/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 10/100\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 11/100\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 12/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 13/100\n",
      "4445/4445 [==============================] - 0s 56us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 14/100\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 15/100\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 16/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "Epoch 17/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 18/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 19/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 20/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 21/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 22/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 23/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 24/100\n",
      "4445/4445 [==============================] - 0s 54us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 25/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 26/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 27/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 28/100\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 29/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 30/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 31/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 32/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 33/100\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 34/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 35/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 36/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 37/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 38/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 39/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9560 - mean_absolute_error: 0.9560\n",
      "Epoch 40/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 41/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 42/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 43/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 44/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 45/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 46/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 47/100\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 48/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 49/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 50/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 51/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 52/100\n",
      "4445/4445 [==============================] - 0s 49us/step - loss: 0.9559 - mean_absolute_error: 0.9559\n",
      "Epoch 53/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 54/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 55/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 56/100\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 57/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 58/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 59/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 60/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9545 - mean_absolute_error: 0.9545\n",
      "Epoch 61/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 62/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 63/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 64/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 65/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 66/100\n",
      "4445/4445 [==============================] - 0s 53us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 67/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 68/100\n",
      "4445/4445 [==============================] - 0s 47us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 69/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 70/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 72/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 73/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 74/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9562 - mean_absolute_error: 0.9562\n",
      "Epoch 75/100\n",
      "4445/4445 [==============================] - 0s 46us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 76/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 77/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 78/100\n",
      "4445/4445 [==============================] - 0s 43us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 79/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 80/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 81/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n",
      "Epoch 82/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 83/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9570 - mean_absolute_error: 0.9570\n",
      "Epoch 84/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9567 - mean_absolute_error: 0.9567\n",
      "Epoch 85/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 86/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 87/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9563 - mean_absolute_error: 0.9563\n",
      "Epoch 88/100\n",
      "4445/4445 [==============================] - 0s 50us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 89/100\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 90/100\n",
      "4445/4445 [==============================] - 0s 48us/step - loss: 0.9548 - mean_absolute_error: 0.9548: 0s - loss: 0.9578 - mean_absolute_error: 0.95\n",
      "Epoch 91/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9556 - mean_absolute_error: 0.9556\n",
      "Epoch 92/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9551 - mean_absolute_error: 0.9551\n",
      "Epoch 93/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9555 - mean_absolute_error: 0.9555\n",
      "Epoch 94/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 95/100\n",
      "4445/4445 [==============================] - 0s 52us/step - loss: 0.9564 - mean_absolute_error: 0.9564\n",
      "Epoch 96/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9553 - mean_absolute_error: 0.9553\n",
      "Epoch 97/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 98/100\n",
      "4445/4445 [==============================] - 0s 44us/step - loss: 0.9549 - mean_absolute_error: 0.9549\n",
      "Epoch 99/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9552 - mean_absolute_error: 0.9552\n",
      "Epoch 100/100\n",
      "4445/4445 [==============================] - 0s 45us/step - loss: 0.9550 - mean_absolute_error: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16cd4aae748>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now run the best optimizer for more epochs\n",
    "early_stopping = EarlyStopping(monitor='mean_absolute_error', patience=100)\n",
    "bst = ModelCheckpoint('C:/Users/johnb/OneDrive/Documents/MSA/Fall 3/fall3orange3/mdl2.hdf5', save_best_only=True, monitor='mean_absolute_error')\n",
    "nnet.compile(optimizer='nadam', loss='mean_absolute_error', metrics=['mae'])\n",
    "nnet.fit(var_train, target_train, epochs=100, callbacks=[early_stopping,bst])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.167656],\n",
       "       [20.167656],\n",
       "       [20.167656],\n",
       "       ...,\n",
       "       [20.167656],\n",
       "       [20.167656],\n",
       "       [20.167656]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best weights and predict\n",
    "nnet.load_weights(filepath='C:/Users/johnb/OneDrive/Documents/MSA/Fall 3/fall3orange3/mdl2.hdf5')\n",
    "nnet_preds = nnet.predict(var_test)\n",
    "nnet_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009208785379043571"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score\n",
    "nnet_score = mean_absolute_error(target_test, nnet_preds)\n",
    "base_score - nnet_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694967012458221"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's kinda bad\n",
    "nnet_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we add squared variables\n",
    "new_var = var\n",
    "for col in var.columns:\n",
    "    if col[0:3] == 'num':\n",
    "        new_var[col + '_squared'] = (var[col] ** 2)\n",
    "        new_var[col] = var[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_train, new_var_test, target_train, target_test = train_test_split(new_var, target, test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = new_var_train.shape[1]\n",
    "nnet_new = Sequential()\n",
    "nnet_new.add(Dense(50, activation='tanh', input_shape=(new_cols,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new.add(Dense(32, activation='tanh'))\n",
    "nnet_new.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_new.compile(optimizer='adadelta', loss='mean_absolute_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4445/4445 [==============================] - 1s 215us/step - loss: 9.9834 - mean_absolute_error: 9.9834\n",
      "Epoch 2/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 1.2664 - mean_absolute_error: 1.2664\n",
      "Epoch 3/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9713 - mean_absolute_error: 0.9713\n",
      "Epoch 4/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9629 - mean_absolute_error: 0.9629\n",
      "Epoch 5/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9605 - mean_absolute_error: 0.9605\n",
      "Epoch 6/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9597 - mean_absolute_error: 0.9597\n",
      "Epoch 7/1000\n",
      "4445/4445 [==============================] - 0s 88us/step - loss: 0.9582 - mean_absolute_error: 0.9582\n",
      "Epoch 8/1000\n",
      "4445/4445 [==============================] - 0s 78us/step - loss: 0.9582 - mean_absolute_error: 0.9582\n",
      "Epoch 9/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 10/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9561 - mean_absolute_error: 0.9561\n",
      "Epoch 11/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 12/1000\n",
      "4445/4445 [==============================] - 0s 83us/step - loss: 0.9564 - mean_absolute_error: 0.9564\n",
      "Epoch 13/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9569 - mean_absolute_error: 0.9569\n",
      "Epoch 14/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9565 - mean_absolute_error: 0.9565\n",
      "Epoch 15/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9557 - mean_absolute_error: 0.9557\n",
      "Epoch 16/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 17/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9554 - mean_absolute_error: 0.9554\n",
      "Epoch 18/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 19/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9548 - mean_absolute_error: 0.9548\n",
      "Epoch 20/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 21/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9543 - mean_absolute_error: 0.9543\n",
      "Epoch 22/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 23/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9558 - mean_absolute_error: 0.9558\n",
      "Epoch 24/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9542 - mean_absolute_error: 0.9542\n",
      "Epoch 25/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 26/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9546 - mean_absolute_error: 0.9546\n",
      "Epoch 27/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 28/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9547 - mean_absolute_error: 0.9547\n",
      "Epoch 29/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 30/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9540 - mean_absolute_error: 0.9540\n",
      "Epoch 31/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9541 - mean_absolute_error: 0.9541\n",
      "Epoch 32/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9544 - mean_absolute_error: 0.9544\n",
      "Epoch 33/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 34/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9537 - mean_absolute_error: 0.9537\n",
      "Epoch 35/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9533 - mean_absolute_error: 0.9533: 0s - loss: 0.9275 - mean_absolute_error: 0.9\n",
      "Epoch 36/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 37/1000\n",
      "4445/4445 [==============================] - 0s 82us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 38/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 39/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 40/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 41/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 42/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9539 - mean_absolute_error: 0.9539\n",
      "Epoch 43/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 44/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 45/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9538 - mean_absolute_error: 0.9538\n",
      "Epoch 46/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 47/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9535 - mean_absolute_error: 0.9535\n",
      "Epoch 48/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9532 - mean_absolute_error: 0.9532\n",
      "Epoch 49/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9536 - mean_absolute_error: 0.9536\n",
      "Epoch 50/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9534 - mean_absolute_error: 0.9534\n",
      "Epoch 51/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 52/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 53/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 54/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9531 - mean_absolute_error: 0.9531\n",
      "Epoch 55/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 56/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 57/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9529 - mean_absolute_error: 0.9529\n",
      "Epoch 58/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 59/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 60/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 61/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 62/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 63/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 64/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9526 - mean_absolute_error: 0.9526\n",
      "Epoch 65/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 66/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 67/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 68/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 69/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 70/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 71/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 72/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 73/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 74/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 75/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 76/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 77/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9528 - mean_absolute_error: 0.9528\n",
      "Epoch 78/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9527 - mean_absolute_error: 0.9527\n",
      "Epoch 79/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 80/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 81/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9530 - mean_absolute_error: 0.9530\n",
      "Epoch 82/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 83/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9521 - mean_absolute_error: 0.9521\n",
      "Epoch 84/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9517 - mean_absolute_error: 0.9517\n",
      "Epoch 85/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9523 - mean_absolute_error: 0.9523\n",
      "Epoch 86/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 87/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 88/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 89/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 90/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9524 - mean_absolute_error: 0.9524\n",
      "Epoch 91/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 92/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 93/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 94/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 95/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 96/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 97/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 98/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 99/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 100/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9519 - mean_absolute_error: 0.9519\n",
      "Epoch 101/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 102/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 103/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 104/1000\n",
      "4445/4445 [==============================] - 0s 79us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 105/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9507 - mean_absolute_error: 0.9507\n",
      "Epoch 106/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 107/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 108/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 109/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 110/1000\n",
      "4445/4445 [==============================] - 0s 82us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 111/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 112/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 113/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9520 - mean_absolute_error: 0.9520\n",
      "Epoch 114/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9522 - mean_absolute_error: 0.9522\n",
      "Epoch 115/1000\n",
      "4445/4445 [==============================] - 0s 79us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 116/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 117/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 118/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 119/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9518 - mean_absolute_error: 0.9518\n",
      "Epoch 120/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9516 - mean_absolute_error: 0.9516\n",
      "Epoch 121/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 122/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 123/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 124/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 125/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 126/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 127/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 128/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 129/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 130/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9511 - mean_absolute_error: 0.9511: 0s - loss: 0.9376 - mean_absolute_error: \n",
      "Epoch 131/1000\n",
      "4445/4445 [==============================] - 0s 90us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 132/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 133/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 134/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 135/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 136/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 137/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 138/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 140/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 141/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9507 - mean_absolute_error: 0.9507\n",
      "Epoch 142/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 143/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 144/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 145/1000\n",
      "4445/4445 [==============================] - 0s 84us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 146/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 147/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 148/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 149/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 150/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 151/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 152/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 153/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9515 - mean_absolute_error: 0.9515\n",
      "Epoch 154/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9513 - mean_absolute_error: 0.9513\n",
      "Epoch 155/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9507 - mean_absolute_error: 0.9507\n",
      "Epoch 156/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 157/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 158/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 159/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 160/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 161/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 162/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 163/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9514 - mean_absolute_error: 0.9514\n",
      "Epoch 164/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 165/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 166/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 167/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 168/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 169/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 170/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 171/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 172/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 173/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9511 - mean_absolute_error: 0.9511\n",
      "Epoch 174/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9507 - mean_absolute_error: 0.9507\n",
      "Epoch 175/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 176/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 177/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 178/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 179/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 180/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 181/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 182/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 183/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 184/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 185/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 186/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 187/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 188/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 189/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 190/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 191/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 192/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 193/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 194/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9512 - mean_absolute_error: 0.9512\n",
      "Epoch 195/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 196/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9510 - mean_absolute_error: 0.9510\n",
      "Epoch 197/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 198/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 199/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 200/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 201/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 202/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 203/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 204/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9507 - mean_absolute_error: 0.9507\n",
      "Epoch 205/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 206/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 207/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 208/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 209/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 210/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 211/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 212/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 213/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 214/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 215/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 216/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 217/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 218/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 219/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 220/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 221/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 222/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 223/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 224/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 225/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 226/1000\n",
      "4445/4445 [==============================] - 0s 80us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 227/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 228/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 229/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 230/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 231/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 232/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 233/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 234/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 235/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 236/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 237/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 238/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 239/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 240/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 241/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 242/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 243/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 244/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 245/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 246/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 247/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 248/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 249/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9508 - mean_absolute_error: 0.9508\n",
      "Epoch 250/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 251/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 252/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 253/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 254/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 255/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 256/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 257/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 258/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 259/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 260/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 261/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 262/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 263/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 264/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 265/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 266/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 267/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 268/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 269/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 270/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 271/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 272/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 273/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 274/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 275/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 276/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9504 - mean_absolute_error: 0.9504\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 278/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 279/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 280/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9506 - mean_absolute_error: 0.9506\n",
      "Epoch 281/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9509 - mean_absolute_error: 0.9509\n",
      "Epoch 282/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 283/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 284/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 285/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 286/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 287/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 288/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 289/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 290/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 291/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 292/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 293/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 294/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 295/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 296/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 297/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 298/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 299/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 300/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 301/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 302/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 303/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 304/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 305/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 306/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 307/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 308/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 309/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 310/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 311/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 312/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 313/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 314/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 315/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 316/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 317/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 318/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 319/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 320/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 321/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 322/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 323/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 324/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 325/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 326/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 327/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9502 - mean_absolute_error: 0.9502\n",
      "Epoch 328/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 329/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 330/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 331/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 332/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9505 - mean_absolute_error: 0.9505\n",
      "Epoch 333/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 334/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 335/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 336/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 337/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 338/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 339/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 340/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 341/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 342/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9503 - mean_absolute_error: 0.9503\n",
      "Epoch 343/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 344/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 345/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 346/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 347/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 348/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 349/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 350/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 351/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 352/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 353/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 354/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9497 - mean_absolute_error: 0.9497\n",
      "Epoch 355/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 356/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 357/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 358/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 359/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 360/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 361/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 362/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 363/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 364/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 365/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 366/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 367/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 368/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 369/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 370/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 371/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 372/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 373/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9499 - mean_absolute_error: 0.9499\n",
      "Epoch 374/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 375/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 376/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 377/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 378/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 379/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9496 - mean_absolute_error: 0.9496\n",
      "Epoch 380/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 381/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 382/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 383/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 384/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 385/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 386/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 387/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 388/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 389/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 390/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 391/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 392/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 393/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 394/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 395/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 396/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 397/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 398/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 399/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 400/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 401/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 402/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 403/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 404/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 405/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 406/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 407/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 408/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 409/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 410/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 411/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 412/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 413/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 414/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 415/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 416/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 417/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 418/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 419/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 420/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9501 - mean_absolute_error: 0.9501\n",
      "Epoch 421/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 422/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 423/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 424/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 425/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 426/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 427/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 428/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 429/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 430/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 431/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 432/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 433/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 434/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 435/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 436/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 437/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 438/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 439/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 440/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 441/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 442/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 443/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 444/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 445/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9498 - mean_absolute_error: 0.9498\n",
      "Epoch 446/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 447/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 448/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 449/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 450/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 451/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 452/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 453/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 454/1000\n",
      "4445/4445 [==============================] - 0s 78us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 455/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 456/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 457/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 458/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 459/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 460/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 461/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 462/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 463/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 464/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 465/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 466/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 467/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 468/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 469/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 470/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 471/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 472/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 473/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 474/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 475/1000\n",
      "4445/4445 [==============================] - 0s 78us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 476/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 477/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 478/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 479/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 480/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 481/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9494 - mean_absolute_error: 0.9494\n",
      "Epoch 482/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 483/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 484/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 485/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 486/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 487/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 488/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 489/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 490/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 491/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 492/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 493/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 494/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 495/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9492 - mean_absolute_error: 0.9492\n",
      "Epoch 496/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9490 - mean_absolute_error: 0.9490\n",
      "Epoch 497/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 498/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 499/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 500/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 501/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 502/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 503/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 504/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 505/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 506/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 507/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 508/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 509/1000\n",
      "4445/4445 [==============================] - 0s 85us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 510/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 511/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 512/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 513/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 514/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 515/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 516/1000\n",
      "4445/4445 [==============================] - 0s 81us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 517/1000\n",
      "4445/4445 [==============================] - 0s 79us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 518/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 519/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 520/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9489 - mean_absolute_error: 0.9489\n",
      "Epoch 521/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 522/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 523/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 524/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 525/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 526/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 527/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 528/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 529/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 530/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9495 - mean_absolute_error: 0.9495\n",
      "Epoch 531/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 532/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 533/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 534/1000\n",
      "4445/4445 [==============================] - 0s 73us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 535/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 536/1000\n",
      "4445/4445 [==============================] - 0s 92us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 537/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 538/1000\n",
      "4445/4445 [==============================] - 0s 86us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 539/1000\n",
      "4445/4445 [==============================] - 0s 85us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 540/1000\n",
      "4445/4445 [==============================] - 0s 81us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 541/1000\n",
      "4445/4445 [==============================] - 0s 103us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 542/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 543/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 544/1000\n",
      "4445/4445 [==============================] - 0s 86us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 545/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 546/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 547/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 548/1000\n",
      "4445/4445 [==============================] - 0s 78us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 549/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 550/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9500 - mean_absolute_error: 0.9500\n",
      "Epoch 551/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 552/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 553/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 554/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 555/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 556/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 557/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 558/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 559/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 560/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 561/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 562/1000\n",
      "4445/4445 [==============================] - ETA: 0s - loss: 0.9469 - mean_absolute_error: 0.946 - 0s 65us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 563/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 564/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 565/1000\n",
      "4445/4445 [==============================] - ETA: 0s - loss: 0.9541 - mean_absolute_error: 0.954 - 0s 68us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 566/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 567/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 568/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 569/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 570/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 571/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 572/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 573/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 574/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 575/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 576/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 577/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 578/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 579/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 580/1000\n",
      "4445/4445 [==============================] - 0s 84us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 581/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 582/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 583/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9488 - mean_absolute_error: 0.9488\n",
      "Epoch 584/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 585/1000\n",
      "4445/4445 [==============================] - 0s 80us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 586/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 587/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9493 - mean_absolute_error: 0.9493\n",
      "Epoch 588/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 589/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 590/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9491 - mean_absolute_error: 0.9491\n",
      "Epoch 591/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 592/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 593/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 594/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 595/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 596/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 597/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 598/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 599/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 600/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 601/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 602/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 603/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 604/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 605/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 606/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9485 - mean_absolute_error: 0.9485\n",
      "Epoch 607/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 608/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 609/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 610/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 611/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 612/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 613/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 614/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 615/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 616/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 617/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 618/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 619/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 620/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 622/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 623/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 624/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 625/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9486 - mean_absolute_error: 0.9486\n",
      "Epoch 626/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 627/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 628/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 629/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 630/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 631/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 632/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 633/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 634/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 635/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 636/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 637/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 638/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 639/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 640/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 641/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 642/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 643/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 644/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 645/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 646/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 647/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 648/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 649/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 650/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 651/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 652/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 653/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 654/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 655/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 656/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 657/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 658/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 659/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 660/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 661/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 662/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 663/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 664/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 665/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 666/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 667/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 668/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 669/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 670/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 671/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 672/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 673/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 674/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 675/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 676/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 677/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9487 - mean_absolute_error: 0.9487\n",
      "Epoch 678/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 679/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 680/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 681/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 682/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 683/1000\n",
      "4445/4445 [==============================] - 0s 82us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 684/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 685/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 686/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 687/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 688/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 689/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 690/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 691/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 692/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 693/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 694/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 695/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 696/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 697/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 698/1000\n",
      "4445/4445 [==============================] - ETA: 0s - loss: 0.9425 - mean_absolute_error: 0.942 - 0s 64us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 699/1000\n",
      "4445/4445 [==============================] - 0s 92us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 700/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 701/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 702/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 703/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 704/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 705/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 706/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 707/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 708/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 709/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 710/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 711/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 712/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 713/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 714/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 715/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 716/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 717/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 718/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 719/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 720/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 721/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 722/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 723/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 724/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 725/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 726/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 727/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 728/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 729/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 730/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9461 - mean_absolute_error: 0.9461\n",
      "Epoch 731/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 732/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 733/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 734/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 735/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 736/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 737/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 738/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 739/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 740/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 741/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 742/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 743/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 744/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 745/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 746/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 747/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 748/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 749/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 750/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 751/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 752/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 753/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 754/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 755/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 756/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 757/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 758/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 759/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 760/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 761/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 762/1000\n",
      "4445/4445 [==============================] - 0s 86us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 763/1000\n",
      "4445/4445 [==============================] - 0s 91us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 764/1000\n",
      "4445/4445 [==============================] - 0s 85us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 765/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 766/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 767/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 768/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 769/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 770/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 771/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 772/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 773/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 774/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 775/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 776/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 777/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 778/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 779/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 780/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 781/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 782/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 783/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 784/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 785/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9460 - mean_absolute_error: 0.9460\n",
      "Epoch 786/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 787/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 788/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 789/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 790/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 791/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 792/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 793/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 794/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 795/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 796/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 797/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 798/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 799/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 800/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 801/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 802/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 803/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 804/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 805/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 806/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 807/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 808/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 809/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 810/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 811/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 812/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9484 - mean_absolute_error: 0.9484\n",
      "Epoch 813/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 814/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 815/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9461 - mean_absolute_error: 0.9461\n",
      "Epoch 816/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9462 - mean_absolute_error: 0.9462\n",
      "Epoch 817/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 818/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 819/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 820/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 821/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 822/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 823/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 824/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 825/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 826/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9461 - mean_absolute_error: 0.9461\n",
      "Epoch 827/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9480 - mean_absolute_error: 0.9480\n",
      "Epoch 828/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 829/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 830/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 831/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 832/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 833/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 834/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9477 - mean_absolute_error: 0.9477\n",
      "Epoch 835/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 836/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9483 - mean_absolute_error: 0.9483\n",
      "Epoch 837/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 838/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 839/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 840/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 841/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 842/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 843/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 844/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 845/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 846/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 847/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 848/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 849/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 850/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 851/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 852/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 853/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 854/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 855/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 856/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 857/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 858/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 859/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 860/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 861/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9462 - mean_absolute_error: 0.9462\n",
      "Epoch 862/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 863/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 864/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 865/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 866/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 867/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 868/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 869/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 870/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 871/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 872/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 873/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 874/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 875/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 876/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 877/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9460 - mean_absolute_error: 0.9460\n",
      "Epoch 878/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 879/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 880/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 881/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 882/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 883/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9479 - mean_absolute_error: 0.9479\n",
      "Epoch 884/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 885/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 886/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 887/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 888/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 889/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 890/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 891/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 892/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 893/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 894/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 895/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9482 - mean_absolute_error: 0.9482\n",
      "Epoch 896/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 897/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 898/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 899/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 900/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 901/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9481 - mean_absolute_error: 0.9481\n",
      "Epoch 902/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 903/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 904/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 905/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 906/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 907/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 908/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 909/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 910/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 911/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 912/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 913/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 914/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 915/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 916/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 917/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 918/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 919/1000\n",
      "4445/4445 [==============================] - 0s 57us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 920/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 921/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 922/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 923/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 924/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9467 - mean_absolute_error: 0.9467\n",
      "Epoch 925/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 926/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 927/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 928/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 929/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 930/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 931/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9454 - mean_absolute_error: 0.9454\n",
      "Epoch 932/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 933/1000\n",
      "4445/4445 [==============================] - 0s 58us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 934/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 935/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9458 - mean_absolute_error: 0.9458\n",
      "Epoch 936/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 937/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 938/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 939/1000\n",
      "4445/4445 [==============================] - 0s 59us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 940/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 941/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 942/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 943/1000\n",
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 944/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 945/1000\n",
      "4445/4445 [==============================] - 0s 61us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 946/1000\n",
      "4445/4445 [==============================] - 0s 81us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 947/1000\n",
      "4445/4445 [==============================] - 0s 87us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 948/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 949/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 950/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 951/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 952/1000\n",
      "4445/4445 [==============================] - 0s 75us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 953/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9476 - mean_absolute_error: 0.9476\n",
      "Epoch 954/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 955/1000\n",
      "4445/4445 [==============================] - 0s 64us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 956/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 957/1000\n",
      "4445/4445 [==============================] - 0s 78us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 958/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 959/1000\n",
      "4445/4445 [==============================] - 0s 60us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 960/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9461 - mean_absolute_error: 0.9461\n",
      "Epoch 961/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 962/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 963/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 964/1000\n",
      "4445/4445 [==============================] - 0s 77us/step - loss: 0.9460 - mean_absolute_error: 0.9460\n",
      "Epoch 965/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9461 - mean_absolute_error: 0.9461\n",
      "Epoch 966/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445/4445 [==============================] - 0s 62us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 967/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9460 - mean_absolute_error: 0.9460\n",
      "Epoch 968/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 969/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 970/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9470 - mean_absolute_error: 0.9470\n",
      "Epoch 971/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 972/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 973/1000\n",
      "4445/4445 [==============================] - 0s 67us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 974/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 975/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 976/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9458 - mean_absolute_error: 0.9458\n",
      "Epoch 977/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9463 - mean_absolute_error: 0.9463\n",
      "Epoch 978/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9475 - mean_absolute_error: 0.9475\n",
      "Epoch 979/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 980/1000\n",
      "4445/4445 [==============================] - 0s 63us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 981/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 982/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 983/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 984/1000\n",
      "4445/4445 [==============================] - 0s 65us/step - loss: 0.9478 - mean_absolute_error: 0.9478\n",
      "Epoch 985/1000\n",
      "4445/4445 [==============================] - 0s 74us/step - loss: 0.9462 - mean_absolute_error: 0.9462\n",
      "Epoch 986/1000\n",
      "4445/4445 [==============================] - 0s 66us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 987/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 988/1000\n",
      "4445/4445 [==============================] - 0s 72us/step - loss: 0.9459 - mean_absolute_error: 0.9459\n",
      "Epoch 989/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9464 - mean_absolute_error: 0.9464\n",
      "Epoch 990/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9462 - mean_absolute_error: 0.9462\n",
      "Epoch 991/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9468 - mean_absolute_error: 0.9468\n",
      "Epoch 992/1000\n",
      "4445/4445 [==============================] - 0s 70us/step - loss: 0.9471 - mean_absolute_error: 0.9471\n",
      "Epoch 993/1000\n",
      "4445/4445 [==============================] - 0s 71us/step - loss: 0.9474 - mean_absolute_error: 0.9474\n",
      "Epoch 994/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 995/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9473 - mean_absolute_error: 0.9473\n",
      "Epoch 996/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n",
      "Epoch 997/1000\n",
      "4445/4445 [==============================] - 0s 69us/step - loss: 0.9472 - mean_absolute_error: 0.9472\n",
      "Epoch 998/1000\n",
      "4445/4445 [==============================] - 0s 76us/step - loss: 0.9465 - mean_absolute_error: 0.9465\n",
      "Epoch 999/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9469 - mean_absolute_error: 0.9469\n",
      "Epoch 1000/1000\n",
      "4445/4445 [==============================] - 0s 68us/step - loss: 0.9466 - mean_absolute_error: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16cd0fee320>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet_new.fit(new_var_train, target_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_preds_new = nnet_new.predict(new_var_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009208785379043571"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet_score_new = mean_absolute_error(target_test, nnet_preds)\n",
    "base_score - nnet_score_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694967012458221"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still pretty bad\n",
    "nnet_score_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
